{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f67ede",
   "metadata": {},
   "source": [
    "# Running Inference\n",
    "\n",
    "This notebook demonstrates how to run inference with both PyTorch and TensorRT models, benchmark their performance, and verify that both models produce similar outputs. It includes:\n",
    "\n",
    "- Loading PyTorch and TensorRT models\n",
    "- Performance benchmarking (latency comparison)\n",
    "- Output verification (comparing translations and numerical outputs)\n",
    "- Encoder output accuracy verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import get_config, get_weights_file_path\n",
    "from train import get_model, get_ds, run_validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098d6eb",
   "metadata": {},
   "source": [
    "## Setup: Device and Data Loading\n",
    "\n",
    "Initialize the device (CUDA if available), load configuration, and prepare the data loaders and tokenizers for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bfdf99",
   "metadata": {},
   "source": [
    "## Load PyTorch Model\n",
    "\n",
    "Load the trained PyTorch transformer model from the checkpoint. The model is moved to the specified device (CUDA/CPU) and weights are loaded from the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = get_weights_file_path(config, f\"10\")\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34b7ae",
   "metadata": {},
   "source": [
    "## Prepare for Inference\n",
    "\n",
    "Set the model to evaluation mode and disable gradient computation for faster inference. Import libraries needed for benchmarking (time, numpy, pandas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaeeaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bb38b",
   "metadata": {},
   "source": [
    "## Load TensorRT Model\n",
    "\n",
    "Load the split TensorRT engines (encoder, decoder, and projection layers). The memory fraction is limited to 60% to ensure TensorRT has enough GPU memory. The engines are loaded from the `tensorrt_split/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: this import should NOT trigger inference because the file uses if __name__ == \"__main__\"\n",
    "from run_trt_split import TRTTransformer, greedy_decode as greedy_decode_trt\n",
    "\n",
    "# (optional) same memory fraction trick in notebook\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_per_process_memory_fraction(0.6)\n",
    "\n",
    "trt_model = TRTTransformer(\n",
    "    enc_path=\"tensorrt_split/tmodel_10_encoder_fp32.engine\",\n",
    "    dec_path=\"tensorrt_split/tmodel_10_decoder_fp32.engine\",\n",
    "    proj_path=\"tensorrt_split/tmodel_10_projection_fp32.engine\"\n",
    ")\n",
    "\n",
    "trt_model  # sanity: should print \"Engines loaded.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431ebf4",
   "metadata": {},
   "source": [
    "## Import Decoding Functions\n",
    "\n",
    "Import the greedy decoding function from the training module for PyTorch inference. The TensorRT version was already imported above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import greedy_decode as greedy_decode_pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a9e79",
   "metadata": {},
   "source": [
    "## Benchmark Function\n",
    "\n",
    "The `benchmark_decode` function measures inference latency for a given model and decoding function. It:\n",
    "\n",
    "1. **Warmup Phase**: Runs a few inference passes to stabilize GPU performance and optionally prints sample translations\n",
    "2. **Timed Phase**: Measures actual inference time for multiple batches\n",
    "3. **Statistics**: Computes mean, median (p50), p90, and p99 percentiles of latency\n",
    "\n",
    "This function is used to compare PyTorch vs TensorRT performance.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a711112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_decode(\n",
    "    decode_fn,\n",
    "    model_obj,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"\",\n",
    "    print_warmup_samples=0   # <-- NEW\n",
    "):\n",
    "    times_ms = []\n",
    "\n",
    "    # ---------------- Warmup (optional prints here) ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= warmup_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        out_ids = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "        if i < print_warmup_samples:\n",
    "            out_text = tokenizer_tgt.decode(out_ids.detach().cpu().numpy())\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{label} WARMUP SAMPLE {i+1}\")\n",
    "            print(f\"SOURCE:    {batch['src_text'][0]}\")\n",
    "            print(f\"TARGET:    {batch['tgt_text'][0]}\")\n",
    "            print(f\"PRED:      {out_text}\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # ---------------- Timed runs (no printing) ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        _ = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        times_ms.append((t1 - t0) * 1000)\n",
    "\n",
    "    arr = np.array(times_ms, dtype=np.float32)\n",
    "    print(f\"\\n{label} latency over {len(arr)} batches:\")\n",
    "    print(f\"  mean: {arr.mean():.2f} ms\")\n",
    "    print(f\"  p50 : {np.percentile(arr, 50):.2f} ms\")\n",
    "    print(f\"  p90 : {np.percentile(arr, 90):.2f} ms\")\n",
    "    print(f\"  p99 : {np.percentile(arr, 99):.2f} ms\")\n",
    "\n",
    "    return times_ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d67ad",
   "metadata": {},
   "source": [
    "## Run Performance Benchmarks\n",
    "\n",
    "Benchmark both PyTorch and TensorRT models on the validation dataset. This will:\n",
    "\n",
    "- Run 5 warmup batches (with 2 sample translations printed)\n",
    "- Measure latency for 50 batches\n",
    "- Print latency statistics (mean, p50, p90, p99)\n",
    "- Calculate the speedup factor (PyTorch latency / TensorRT latency)\n",
    "\n",
    "The results show how much faster TensorRT is compared to PyTorch for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eabe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_len = config[\"seq_len\"]\n",
    "\n",
    "pt_times = benchmark_decode(\n",
    "    greedy_decode_pt, model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"PyTorch\",\n",
    "    print_warmup_samples=2\n",
    ")\n",
    "\n",
    "trt_times = benchmark_decode(\n",
    "    greedy_decode_trt, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"TensorRT\",\n",
    "    print_warmup_samples=2\n",
    ")\n",
    "\n",
    "speedup = np.mean(pt_times) / np.mean(trt_times)\n",
    "print(f\"\\nSpeedup (PyTorch / TRT): {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f0bc4",
   "metadata": {},
   "source": [
    "## Translation Comparison\n",
    "\n",
    "The `verbose_compare_same_sample` function compares translations from both models on the same input sample. It:\n",
    "\n",
    "- Takes the same batch from the dataloader\n",
    "- Runs inference with both PyTorch and TensorRT models\n",
    "- Prints side-by-side comparison of source, target, and predictions\n",
    "- Returns the token IDs from both models\n",
    "\n",
    "This helps verify that both models produce similar (or identical) translations.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8940670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verbose_compare_same_sample(\n",
    "    pt_model,\n",
    "    trt_model,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    batch_index=0,   # pick which batch to compare\n",
    "):\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---- get ONE specific batch ----\n",
    "    it = iter(dataloader)\n",
    "    batch = None\n",
    "    for i in range(batch_index + 1):\n",
    "        batch = next(it)\n",
    "\n",
    "    encoder_input = batch[\"encoder_input\"].to(device)\n",
    "    encoder_mask  = batch[\"encoder_mask\"].to(device)\n",
    "    source_text   = batch[\"src_text\"][0]\n",
    "    target_text   = batch[\"tgt_text\"][0]\n",
    "\n",
    "    # ---- PyTorch decode on SAME tensors ----\n",
    "    out_pt_ids = greedy_decode_pt(\n",
    "        pt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_pt_text = tokenizer_tgt.decode(out_pt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- TensorRT decode on SAME tensors ----\n",
    "    out_trt_ids = greedy_decode_trt(\n",
    "        trt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_trt_text = tokenizer_tgt.decode(out_trt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- print SAME sample ----\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"SOURCE:     {source_text}\")\n",
    "    print(f\"TARGET:     {target_text}\")\n",
    "    print(f\"PT  PRED:   {out_pt_text}\")\n",
    "    print(f\"TRT PRED:   {out_trt_text}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    return out_pt_ids, out_trt_ids\n",
    "\n",
    "\n",
    "# run on batch 0 (same sample)\n",
    "_ = verbose_compare_same_sample(\n",
    "    model, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    config[\"seq_len\"], device,\n",
    "    batch_index=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97e13c",
   "metadata": {},
   "source": [
    "## Logits Comparison (First Decoding Step)\n",
    "\n",
    "The `compare_first_step_logits` function verifies numerical accuracy by comparing the logits from the first decoding step. It:\n",
    "\n",
    "- Encodes the source sequence with both models\n",
    "- Runs one decoder step (with [SOS] token) for both models\n",
    "- Compares the output logits (vocabulary probabilities)\n",
    "- Reports maximum and mean absolute differences\n",
    "\n",
    "Small differences (< 0.01) indicate that TensorRT is producing numerically similar outputs to PyTorch, which is expected due to floating-point precision differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fdf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import causal_mask\n",
    "def compare_first_step_logits(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    # PT: encoder + one decoder step (SOS)\n",
    "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "    dec_in = torch.tensor([[sos_idx]], device=device, dtype=src.dtype)\n",
    "\n",
    "    enc_pt = pt_model.encode(src, src_mask)\n",
    "    dec_mask = causal_mask(1).type_as(src_mask).to(device)  # (1,1,1)\n",
    "    out_pt = pt_model.decode(enc_pt, src_mask, dec_in, dec_mask)\n",
    "    logits_pt = pt_model.project(out_pt[:, -1])  # (B, vocab)\n",
    "\n",
    "    # TRT: same\n",
    "    enc_trt = trt_model.encode(src, src_mask)\n",
    "    dec_mask_trt = causal_mask(1).type_as(src_mask).to(device).unsqueeze(1)  # (1,1,1,1)\n",
    "    out_trt = trt_model.decode(enc_trt, src_mask, dec_in, dec_mask_trt)\n",
    "    logits_trt = trt_model.project(out_trt[:, -1])\n",
    "\n",
    "    lp = logits_pt.detach().cpu().float()\n",
    "    lt = logits_trt.detach().cpu().float()\n",
    "\n",
    "    max_abs = (lp - lt).abs().max().item()\n",
    "    mean_abs = (lp - lt).abs().mean().item()\n",
    "    print(\"First-step logits diff:\")\n",
    "    print(\"  max abs:\", max_abs)\n",
    "    print(\"  mean abs:\", mean_abs)\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_first_step_logits(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0b996",
   "metadata": {},
   "source": [
    "## Encoder Output Comparison\n",
    "\n",
    "The `compare_encoder_outputs` function verifies that the encoder produces similar outputs in both models. It:\n",
    "\n",
    "- **`pt_encode_trt_style`**: Adapts PyTorch encoder to use the same mask format as TensorRT (square mask shape)\n",
    "- **`compare_encoder_outputs`**: Compares encoder outputs from both models and reports maximum and mean absolute differences\n",
    "\n",
    "This ensures that the encoder component is working correctly in the TensorRT version. Differences should be very small (< 0.001) due to floating-point precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_encode_trt_style(pt_model, src, src_mask):\n",
    "    # Make PT use the same square mask shape TRT encoder expects\n",
    "    S = src.shape[1]\n",
    "    if src_mask.dim() == 3:\n",
    "        src_mask = src_mask.unsqueeze(1)   # (B,1,S)->(B,1,1,S)\n",
    "    if src_mask.dim() == 4 and src_mask.shape[2] == 1:\n",
    "        src_mask = src_mask.repeat(1,1,S,1)  # -> (B,1,S,S)\n",
    "    src_mask = src_mask.float()  # binary float\n",
    "    return pt_model.encode(src, src_mask)\n",
    "\n",
    "def compare_encoder_outputs(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    enc_pt  = pt_encode_trt_style(pt_model, src, src_mask).detach().cpu().float()\n",
    "    enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "    diff = (enc_pt - enc_trt).abs()\n",
    "    print(\"ENCODER diff: max =\", diff.max().item(), \"mean =\", diff.mean().item())\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_encoder_outputs(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8351e3",
   "metadata": {},
   "source": [
    "## Position-wise Encoder Analysis\n",
    "\n",
    "This cell performs a detailed analysis of encoder output differences:\n",
    "\n",
    "- Computes per-position mean differences across the hidden dimension\n",
    "- Separates differences for **padded positions** (where mask = 0) vs **unpadded positions** (where mask = 1)\n",
    "- Reports mean differences for each category and the overall maximum\n",
    "\n",
    "This helps understand if differences are concentrated in padded regions (which are typically ignored) or in actual content positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "batch0 = next(iter(val_dataloader))\n",
    "src = batch0[\"encoder_input\"].to(device)\n",
    "src_mask = batch0[\"encoder_mask\"].to(device)\n",
    "\n",
    "enc_pt  = pt_encode_trt_style(model, src, src_mask).detach().cpu().float()\n",
    "enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "# per-position mean diff across hidden dim\n",
    "pos_diff = (enc_pt - enc_trt).abs().mean(-1).squeeze(0)   # (S,)\n",
    "\n",
    "# padded vs unpadded positions (mask is (1,1,1,S))\n",
    "key_mask = src_mask.squeeze().cpu()  # (S,) with 0/1\n",
    "pad_pos = key_mask == 0\n",
    "unpad_pos = key_mask == 1\n",
    "\n",
    "print(\"mean diff padded   :\", pos_diff[pad_pos].mean().item())\n",
    "print(\"mean diff unpadded :\", pos_diff[unpad_pos].mean().item())\n",
    "print(\"max diff overall   :\", pos_diff.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a98e9",
   "metadata": {},
   "source": [
    "## Export Benchmark Results\n",
    "\n",
    "Save the benchmark timing data to a CSV file for further analysis. The DataFrame contains:\n",
    "\n",
    "- `pytorch_ms`: Latency for each PyTorch inference (milliseconds)\n",
    "- `tensorrt_ms`: Latency for each TensorRT inference (milliseconds)\n",
    "- `speedup_x`: Per-sample speedup ratio (PyTorch / TensorRT)\n",
    "\n",
    "The `describe()` method provides summary statistics (mean, std, min, max, percentiles) for all columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3569b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = min(len(pt_times), len(trt_times))\n",
    "df = pd.DataFrame({\n",
    "    \"pytorch_ms\": pt_times[:m],\n",
    "    \"tensorrt_ms\": trt_times[:m],\n",
    "})\n",
    "df[\"speedup_x\"] = df[\"pytorch_ms\"] / df[\"tensorrt_ms\"]\n",
    "\n",
    "df.to_csv(\"benchmark_times.csv\", index=False)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36673b4b",
   "metadata": {},
   "source": [
    "## Optional: Full Validation\n",
    "\n",
    "Uncomment the line below to run full validation on the PyTorch model. This will print multiple translation examples with source, target, and predicted outputs. This is useful for qualitative assessment of translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31231d48",
   "metadata": {},
   "source": [
    "# Checking the tensorboard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are on ssh then make sure you are doing port forwarding\n",
    "#ssh -L 6005:localhost:6005 user@jetson_ip\n",
    "#Finally on your host on a browser open http://localhost:6005\n",
    "\n",
    "logdir = \"./runs/tmodel/\"\n",
    "os.system(f\"tensorboard --logdir {logdir} --port 6005\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
