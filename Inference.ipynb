{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f67ede",
   "metadata": {},
   "source": [
    "# Running Inference\n",
    "\n",
    "This notebook demonstrates how to run inference with both PyTorch and TensorRT models, benchmark their performance, and verify that both models produce similar outputs. It includes:\n",
    "\n",
    "- Loading PyTorch and TensorRT models\n",
    "- Performance benchmarking (latency comparison)\n",
    "- Output verification (comparing translations and numerical outputs)\n",
    "- Encoder output accuracy verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac52c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guru/git_reps/transformer_pytorch/myenv_jetson/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from config import get_config, get_weights_file_path\n",
    "from train import get_model, get_ds, run_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098d6eb",
   "metadata": {},
   "source": [
    "## Setup: Device and Data Loading\n",
    "\n",
    "Initialize the device (CUDA if available), load configuration, and prepare the data loaders and tokenizers for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2d1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Max length of source sentence: 466\n",
      "Max length of target sentence: 479\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bfdf99",
   "metadata": {},
   "source": [
    "## Load PyTorch Model\n",
    "\n",
    "Load the trained PyTorch transformer model from the checkpoint. The model is moved to the specified device (CUDA/CPU) and weights are loaded from the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd12198b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = get_weights_file_path(config, f\"13\")\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34b7ae",
   "metadata": {},
   "source": [
    "## Prepare for Inference\n",
    "\n",
    "Set the model to evaluation mode and disable gradient computation for faster inference. Import libraries needed for benchmarking (time, numpy, pandas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edaeeaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bb38b",
   "metadata": {},
   "source": [
    "## Load TensorRT Model\n",
    "\n",
    "Load the split TensorRT engines (encoder, decoder, and projection layers). The memory fraction is limited to 60% to ensure TensorRT has enough GPU memory. The engines are loaded from the `tensorrt_split/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59c40dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRT Engines...\n",
      "Engines loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<run_trt_split.TRTTransformer at 0xffff922e2800>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: this import should NOT trigger inference because the file uses if __name__ == \"__main__\"\n",
    "from run_trt_split import TRTTransformer, greedy_decode as greedy_decode_trt\n",
    "\n",
    "# (optional) same memory fraction trick in notebook\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_per_process_memory_fraction(0.6)\n",
    "\n",
    "trt_model = TRTTransformer(\n",
    "    enc_path=\"tensorrt_split/tmodel_13_encoder_fp32.engine\",\n",
    "    dec_path=\"tensorrt_split/tmodel_13_decoder_fp32.engine\",\n",
    "    proj_path=\"tensorrt_split/tmodel_13_projection_fp32.engine\"\n",
    ")\n",
    "\n",
    "trt_model  # sanity: should print \"Engines loaded.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431ebf4",
   "metadata": {},
   "source": [
    "## Import Decoding Functions\n",
    "\n",
    "Import the greedy decoding function from the training module for PyTorch inference. The TensorRT version was already imported above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21fa68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import greedy_decode as greedy_decode_pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a9e79",
   "metadata": {},
   "source": [
    "## Benchmark Function\n",
    "\n",
    "The `benchmark_decode` function measures inference latency for a given model and decoding function. It:\n",
    "\n",
    "1. **Warmup Phase**: Runs a few inference passes to stabilize GPU performance and optionally prints sample translations\n",
    "2. **Timed Phase**: Measures actual inference time for multiple batches\n",
    "3. **Statistics**: Computes mean, median (p50), p90, and p99 percentiles of latency\n",
    "\n",
    "This function is used to compare PyTorch vs TensorRT performance.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a711112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_decode(\n",
    "    decode_fn,\n",
    "    model_obj,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"\",\n",
    "    print_warmup_samples=0 \n",
    "):\n",
    "    times_ms = []\n",
    "\n",
    "    # ---------------- Warmup (optional prints here) ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= warmup_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        out_ids = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "        if i < print_warmup_samples:\n",
    "            out_text = tokenizer_tgt.decode(out_ids.detach().cpu().numpy())\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{label} WARMUP SAMPLE {i+1}\")\n",
    "            print(f\"SOURCE:    {batch['src_text'][0]}\")\n",
    "            print(f\"TARGET:    {batch['tgt_text'][0]}\")\n",
    "            print(f\"PRED:      {out_text}\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # ---------------- Timed runs ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        _ = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        times_ms.append((t1 - t0) * 1000)\n",
    "\n",
    "    arr = np.array(times_ms, dtype=np.float32)\n",
    "    print(f\"\\n{label} latency over {len(arr)} batches:\")\n",
    "    print(f\"  mean: {arr.mean():.2f} ms\")\n",
    "    print(f\"  p50 : {np.percentile(arr, 50):.2f} ms\")\n",
    "    print(f\"  p90 : {np.percentile(arr, 90):.2f} ms\")\n",
    "    print(f\"  p99 : {np.percentile(arr, 99):.2f} ms\")\n",
    "\n",
    "    return times_ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d67ad",
   "metadata": {},
   "source": [
    "## Run Performance Benchmarks\n",
    "\n",
    "Benchmark both PyTorch and TensorRT models on the validation dataset. This will:\n",
    "\n",
    "- Run 5 warmup batches (with 2 sample translations printed)\n",
    "- Measure latency for 50 batches\n",
    "- Print latency statistics (mean, p50, p90, p99)\n",
    "- Calculate the speedup factor (PyTorch latency / TensorRT latency)\n",
    "\n",
    "The results show how much faster TensorRT is compared to PyTorch for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5eabe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 1\n",
      "SOURCE:    He reproached himself with forgetting Emma, as if, all his thoughts belonging to this woman, it was robbing her of something not to be constantly thinking of her.\n",
      "TARGET:    Er redete sich ein, er vernachlässige seine Frau, wenn er ihr nicht all sein Dichten und Trachten widme. Er wollte an nichts andres denken, selbst wenn ihr dadurch kein Abbruch geschähe.\n",
      "PRED:      Er hatte sich über Emma häufig vergessen , als sei es ihm , daß sein Weib , die , die sie sich immer noch mehr an sich selbst gewöhnt gehabt hatten .\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 2\n",
      "SOURCE:    Gringoire stooped quickly to pick it up; when he straightened up, the young girl and the goat had disappeared.\n",
      "TARGET:    Gringoire bückte sich schnell, um es aufzuheben; als er sich wieder erhob, waren das junge Mädchen und die Ziege verschwenden.\n",
      "PRED:      Gringoire neigte sich rasch zu den Füßen hin , um ihn zu holen ; als er sich erhob , und das junge Mädchen war die Ziege verschwunden .\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 3\n",
      "SOURCE:    What most displeased him was that Golenishchev, a man belonging to good Society, should put himself an the same level with certain scribblers who irritated him and made him angry.\n",
      "TARGET:    Besonders mißfiel es ihm, daß Golenischtschew, ein Mann von gutem Stande, sich mit irgendwelchen Literaten, die ihn angriffen, auf eine Stufe stellte und sich über sie ärgerte.\n",
      "PRED:      Was ihn betraf , so war Golenischtschew , der gute Mann , der sich mit einer gewissen Kraft und ihm .\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 4\n",
      "SOURCE:    \"Come to me--come to me entirely now,\" said he; and added, in his deepest tone, speaking in my ear as his cheek was laid on mine, \"Make my happiness--I will make yours.\"\n",
      "TARGET:    »Kommen Sie zu mir – kommen Sie für Zeit und Ewigkeit zu mir,« sagte er und fügte in seinem innigsten Tone hinzu, indem er seine Wange an die meine legte und mir ins Ohr flüsterte: »Mach du mein Glück – ich werde das deine machen.«\n",
      "PRED:      » Nun , nun , nun kommen Sie ganz und gar nicht ,« sagte er , und fügte in seinem Ton hinzu , indem er leise den Ton in mein Ohr zog , » mein Glück , ich will dich auf die meine Wange legen .«\n",
      "\n",
      "PyTorch latency over 50 batches:\n",
      "  mean: 932.02 ms\n",
      "  p50 : 699.72 ms\n",
      "  p90 : 1884.21 ms\n",
      "  p99 : 2608.41 ms\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 1\n",
      "SOURCE:    Throughout the story, his duty seems to have been merely to turn the man away, and there are many commentators who are surprised that the doorkeeper offered this hint at all, as he seems to love exactitude and keeps strict guard over his position.\n",
      "TARGET:    Zu jener Zeit scheint es nur seine Pflicht gewesen zu sein, den Mann abzuweisen, und tatsächlich wundern sich viele Erklärer der Schrift darüber, daß der Türhüter jene Andeutung überhaupt gemacht hat, denn er scheint die Genauigkeit zu lieben und wacht streng über sein Amt.\n",
      "PRED:      Wie stets die Geschichte , die er für sich hielt , scheint , daß der Mann von der Wahrheit und vielen Menschen , die der Türhüter , der die Absicht , über die Liebe , so sehr Liebe , wie er sich über seinen Hut und seiner Person .\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 2\n",
      "SOURCE:    The crowd filed up the aisles: the aged and needy postmaster, who had seen better days; the mayor and his wife--for they had a mayor there, among other unnecessaries; the justice of the peace; the widow Douglass, fair, smart, and forty, a generous, good-hearted soul and well-to-do, her hill mansion the only palace in the town, and the most hospitable and much the most lavish in the matter of festivities that St. Petersburg could boast; the bent and venerable Major and Mrs. Ward; lawyer Riverson, the new notable from a distance; next the belle of the village, followed by a troop of lawn-clad and ribbon-decked young heart-breakers; then all the young clerks in town in a body--for they had stood in the vestibule sucking their cane-heads, a circling wall of oiled and simpering admirers, till the last girl had run their gantlet; and last of all came the Model Boy, Willie Mufferson, taking as heedful care of his mother as if she were cut glass.\n",
      "TARGET:    Das Volk füllte die Kirche. Der alte, gichtbrüchige Postmeister, der bessere Tage gesehen hatte, der Mayor und seine Frau -- denn es gab einen Mayor, neben vielen anderen unnützen Dingen, -- der Ortsrichter, die Witwe Douglas, zart, klein und lebhaft, eine edle, gutherzige Seele und immer obenauf (ihr Haus war das einzige steinerne im Dorf, und das gastfreieste und bei Festlichkeiten verschwenderischste, das St. Petersburg aufweisen konnte); Lawyer Riverson; dann die Schönheit des Dorfes, gefolgt von einem Haufen elegant gekleideter, mit allerhand Firlefanz behangener junger Herzensbrecher; dann all die jungen Ladendiener des Dorfes, alle gleichzeitig, denn sie hatten im Vestibül gestanden, Süßholz raspelnd -- eine öltriefende, einfältige Schutztruppe -- bis das letzte Mädchen Spießruten gelaufen war.\n",
      "PRED:      Die Menge der großen , die das alte und die hatten , die der Alte besser gesehen hatten , hatten für seine Frau , die anderen die Witwe , unter dem andern die Witwe der Witwe der Witwe der Witwe der Witwe der Witwe der Witwe der Witwe der Witwe der Witwe , und nur sehr schöne , , bis sie in der Kirche , , bis alle , bis alle , , , , , , , , , bis sie in der , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , die , , , , die , , die , die , die , die , die , die , die , die , die , die , die , die , die , die , , , die , die , die , die , die , die , die , die , die , die , die , die , die , die , und , die , die , die , die , , die\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 3\n",
      "SOURCE:    He again thought he noticed something in the smile and conquering air with which the visitor addressed Kitty.\n",
      "TARGET:    Er fühlte sich wieder unangenehm berührt durch Weslowskis Lächeln und durch die Siegermiene, mit der er sich zu Kitty gewandt hatte ...\n",
      "PRED:      Er hatte wieder das Gefühl , das er mit Wronski angesehen und mit den Augen zu verbergen , mit dem er Kitty sprach .\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 4\n",
      "SOURCE:    The thought of her son at once took Anna out of the hopeless condition she had been in.\n",
      "TARGET:    Durch den Gedanken an ihren Sohn wurde Anna auf einmal aus der Verzweiflung herausgerissen, in der sie sich über ihre Lage befand.\n",
      "PRED:      Der Gedanke an ihren Sohn , den sie soeben aus dem , war in Verzweiflung gewesen .\n",
      "\n",
      "TensorRT latency over 50 batches:\n",
      "  mean: 619.68 ms\n",
      "  p50 : 253.01 ms\n",
      "  p90 : 692.33 ms\n",
      "  p99 : 7936.96 ms\n",
      "\n",
      "Speedup (PyTorch / TRT): 1.50x\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_len = config[\"seq_len\"]\n",
    "\n",
    "pt_times = benchmark_decode(\n",
    "    greedy_decode_pt, model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"PyTorch\",\n",
    "    print_warmup_samples=4\n",
    ")\n",
    "\n",
    "trt_times = benchmark_decode(\n",
    "    greedy_decode_trt, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"TensorRT\",\n",
    "    print_warmup_samples=4\n",
    ")\n",
    "\n",
    "speedup = np.mean(pt_times) / np.mean(trt_times)\n",
    "print(f\"\\nSpeedup (PyTorch / TRT): {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f0bc4",
   "metadata": {},
   "source": [
    "## Translation Comparison\n",
    "\n",
    "The `verbose_compare_same_sample` function compares translations from both models on the same input sample. It:\n",
    "\n",
    "- Takes the same batch from the dataloader\n",
    "- Runs inference with both PyTorch and TensorRT models\n",
    "- Prints side-by-side comparison of source, target, and predictions\n",
    "- Returns the token IDs from both models\n",
    "\n",
    "This helps verify that both models produce similar (or identical) translations.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8940670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE:     Diana and Mary have left you, and Moor House is shut up, and you are so lonely.\n",
      "TARGET:     Diana und Mary haben Sie verlassen; Moor-House ist verschlossen, und Sie sind einsam.\n",
      "PT  PRED:   Mary und Mary haben dich verlassen , und Moor - House ist geschlossen . Und du bist so einsam .\n",
      "TRT PRED:   Mary und Mary haben dich verlassen , und Moor - House ist geschlossen . Und du bist so einsam .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def verbose_compare_same_sample(\n",
    "    pt_model,\n",
    "    trt_model,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    batch_index=0,   # pick which batch to compare\n",
    "):\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---- get ONE specific batch ----\n",
    "    it = iter(dataloader)\n",
    "    batch = None\n",
    "    for i in range(batch_index + 1):\n",
    "        batch = next(it)\n",
    "\n",
    "    encoder_input = batch[\"encoder_input\"].to(device)\n",
    "    encoder_mask  = batch[\"encoder_mask\"].to(device)\n",
    "    source_text   = batch[\"src_text\"][0]\n",
    "    target_text   = batch[\"tgt_text\"][0]\n",
    "\n",
    "    # ---- PyTorch decode on SAME tensors ----\n",
    "    out_pt_ids = greedy_decode_pt(\n",
    "        pt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_pt_text = tokenizer_tgt.decode(out_pt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- TensorRT decode on SAME tensors ----\n",
    "    out_trt_ids = greedy_decode_trt(\n",
    "        trt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_trt_text = tokenizer_tgt.decode(out_trt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- print SAME sample ----\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"SOURCE:     {source_text}\")\n",
    "    print(f\"TARGET:     {target_text}\")\n",
    "    print(f\"PT  PRED:   {out_pt_text}\")\n",
    "    print(f\"TRT PRED:   {out_trt_text}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    return out_pt_ids, out_trt_ids\n",
    "\n",
    "\n",
    "# run on batch 0 (same sample)\n",
    "_ = verbose_compare_same_sample(\n",
    "    model, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    config[\"seq_len\"], device,\n",
    "    batch_index=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97e13c",
   "metadata": {},
   "source": [
    "## Logits Comparison (First Decoding Step)\n",
    "\n",
    "The `compare_first_step_logits` function verifies numerical accuracy by comparing the logits from the first decoding step. It:\n",
    "\n",
    "- Encodes the source sequence with both models\n",
    "- Runs one decoder step (with [SOS] token) for both models\n",
    "- Compares the output logits (vocabulary probabilities)\n",
    "- Reports maximum and mean absolute differences\n",
    "\n",
    "Small differences (< 0.01) indicate that TensorRT is producing numerically similar outputs to PyTorch, which is expected due to floating-point precision differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6fdf7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-step logits diff:\n",
      "  max abs: 0.007893800735473633\n",
      "  mean abs: 0.0017762379720807076\n"
     ]
    }
   ],
   "source": [
    "from dataset import causal_mask\n",
    "def compare_first_step_logits(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    # PT: encoder + one decoder step (SOS)\n",
    "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "    dec_in = torch.tensor([[sos_idx]], device=device, dtype=src.dtype)\n",
    "\n",
    "    enc_pt = pt_model.encode(src, src_mask)\n",
    "    dec_mask = causal_mask(1).type_as(src_mask).to(device)  # (1,1,1)\n",
    "    out_pt = pt_model.decode(enc_pt, src_mask, dec_in, dec_mask)\n",
    "    logits_pt = pt_model.project(out_pt[:, -1])  # (B, vocab)\n",
    "\n",
    "    # TRT: same\n",
    "    enc_trt = trt_model.encode(src, src_mask)\n",
    "    dec_mask_trt = causal_mask(1).type_as(src_mask).to(device).unsqueeze(1)  # (1,1,1,1)\n",
    "    out_trt = trt_model.decode(enc_trt, src_mask, dec_in, dec_mask_trt)\n",
    "    logits_trt = trt_model.project(out_trt[:, -1])\n",
    "\n",
    "    lp = logits_pt.detach().cpu().float()\n",
    "    lt = logits_trt.detach().cpu().float()\n",
    "\n",
    "    max_abs = (lp - lt).abs().max().item()\n",
    "    mean_abs = (lp - lt).abs().mean().item()\n",
    "    print(\"First-step logits diff:\")\n",
    "    print(\"  max abs:\", max_abs)\n",
    "    print(\"  mean abs:\", mean_abs)\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_first_step_logits(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0b996",
   "metadata": {},
   "source": [
    "## Encoder Output Comparison\n",
    "\n",
    "The `compare_encoder_outputs` function verifies that the encoder produces similar outputs in both models. It:\n",
    "\n",
    "- **`pt_encode_trt_style`**: Adapts PyTorch encoder to use the same mask format as TensorRT (square mask shape)\n",
    "- **`compare_encoder_outputs`**: Compares encoder outputs from both models and reports maximum and mean absolute differences\n",
    "\n",
    "This ensures that the encoder component is working correctly in the TensorRT version. Differences should be very small (< 0.001) due to floating-point precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c220954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER diff: max = 0.000983595848083496 mean = 7.902805373305455e-05\n"
     ]
    }
   ],
   "source": [
    "def pt_encode_trt_style(pt_model, src, src_mask):\n",
    "    # Make PT use the same square mask shape TRT encoder expects\n",
    "    S = src.shape[1]\n",
    "    if src_mask.dim() == 3:\n",
    "        src_mask = src_mask.unsqueeze(1)   # (B,1,S)->(B,1,1,S)\n",
    "    if src_mask.dim() == 4 and src_mask.shape[2] == 1:\n",
    "        src_mask = src_mask.repeat(1,1,S,1)  # -> (B,1,S,S)\n",
    "    src_mask = src_mask.float()  # binary float\n",
    "    return pt_model.encode(src, src_mask)\n",
    "\n",
    "def compare_encoder_outputs(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    enc_pt  = pt_encode_trt_style(pt_model, src, src_mask).detach().cpu().float()\n",
    "    enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "    diff = (enc_pt - enc_trt).abs()\n",
    "    print(\"ENCODER diff: max =\", diff.max().item(), \"mean =\", diff.mean().item())\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_encoder_outputs(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8351e3",
   "metadata": {},
   "source": [
    "## Position-wise Encoder Analysis\n",
    "\n",
    "This cell performs a detailed analysis of encoder output differences:\n",
    "\n",
    "- Computes per-position mean differences across the hidden dimension\n",
    "- Separates differences for **padded positions** (where mask = 0) vs **unpadded positions** (where mask = 1)\n",
    "- Reports mean differences for each category and the overall maximum\n",
    "\n",
    "This helps understand if differences are concentrated in padded regions (which are typically ignored) or in actual content positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96bb3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean diff padded   : 8.144229650497437e-05\n",
      "mean diff unpadded : 3.956778527935967e-05\n",
      "max diff overall   : 0.00023580221750307828\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "batch0 = next(iter(val_dataloader))\n",
    "src = batch0[\"encoder_input\"].to(device)\n",
    "src_mask = batch0[\"encoder_mask\"].to(device)\n",
    "\n",
    "enc_pt  = pt_encode_trt_style(model, src, src_mask).detach().cpu().float()\n",
    "enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "# per-position mean diff across hidden dim\n",
    "pos_diff = (enc_pt - enc_trt).abs().mean(-1).squeeze(0)   # (S,)\n",
    "\n",
    "# padded vs unpadded positions (mask is (1,1,1,S))\n",
    "key_mask = src_mask.squeeze().cpu()  # (S,) with 0/1\n",
    "pad_pos = key_mask == 0\n",
    "unpad_pos = key_mask == 1\n",
    "\n",
    "print(\"mean diff padded   :\", pos_diff[pad_pos].mean().item())\n",
    "print(\"mean diff unpadded :\", pos_diff[unpad_pos].mean().item())\n",
    "print(\"max diff overall   :\", pos_diff.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a98e9",
   "metadata": {},
   "source": [
    "## Export Benchmark Results\n",
    "\n",
    "Save the benchmark timing data to a CSV file for further analysis. The DataFrame contains:\n",
    "\n",
    "- `pytorch_ms`: Latency for each PyTorch inference (milliseconds)\n",
    "- `tensorrt_ms`: Latency for each TensorRT inference (milliseconds)\n",
    "- `speedup_x`: Per-sample speedup ratio (PyTorch / TensorRT)\n",
    "\n",
    "The `describe()` method provides summary statistics (mean, std, min, max, percentiles) for all columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3569b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pytorch_ms</th>\n",
       "      <th>tensorrt_ms</th>\n",
       "      <th>speedup_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>932.016664</td>\n",
       "      <td>619.675165</td>\n",
       "      <td>4.612743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>621.100976</td>\n",
       "      <td>1727.937622</td>\n",
       "      <td>5.182153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>236.616705</td>\n",
       "      <td>84.566726</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>464.246127</td>\n",
       "      <td>155.562562</td>\n",
       "      <td>1.278591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>699.716414</td>\n",
       "      <td>253.012468</td>\n",
       "      <td>2.540888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1252.439549</td>\n",
       "      <td>419.665658</td>\n",
       "      <td>6.157211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2660.253322</td>\n",
       "      <td>12050.381410</td>\n",
       "      <td>22.153174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pytorch_ms   tensorrt_ms  speedup_x\n",
       "count    50.000000     50.000000  50.000000\n",
       "mean    932.016664    619.675165   4.612743\n",
       "std     621.100976   1727.937622   5.182153\n",
       "min     236.616705     84.566726   0.031000\n",
       "25%     464.246127    155.562562   1.278591\n",
       "50%     699.716414    253.012468   2.540888\n",
       "75%    1252.439549    419.665658   6.157211\n",
       "max    2660.253322  12050.381410  22.153174"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = min(len(pt_times), len(trt_times))\n",
    "df = pd.DataFrame({\n",
    "    \"pytorch_ms\": pt_times[:m],\n",
    "    \"tensorrt_ms\": trt_times[:m],\n",
    "})\n",
    "df[\"speedup_x\"] = df[\"pytorch_ms\"] / df[\"tensorrt_ms\"]\n",
    "\n",
    "df.to_csv(\"benchmark_times.csv\", index=False)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36673b4b",
   "metadata": {},
   "source": [
    "## Optional: Full Validation\n",
    "\n",
    "Uncomment the line below to run full validation on the PyTorch model. This will print multiple translation examples with source, target, and predicted outputs. This is useful for qualitative assessment of translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31231d48",
   "metadata": {},
   "source": [
    "# Checking the tensorboard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are on ssh then make sure you are doing port forwarding\n",
    "#ssh -L 6005:localhost:6005 user@jetson_ip\n",
    "#Finally on your host on a browser open http://localhost:6005\n",
    "\n",
    "logdir = \"./runs/tmodel/\"\n",
    "os.system(f\"tensorboard --logdir {logdir} --port 6005\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_jetson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
