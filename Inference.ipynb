{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f67ede",
   "metadata": {},
   "source": [
    "# Running Inference\n",
    "\n",
    "This notebook demonstrates how to run inference with both PyTorch and TensorRT models, benchmark their performance, and verify that both models produce similar outputs. It includes:\n",
    "\n",
    "- Loading PyTorch and TensorRT models\n",
    "- Performance benchmarking (latency comparison)\n",
    "- Output verification (comparing translations and numerical outputs)\n",
    "- Encoder output accuracy verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac52c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guru/git_reps/transformer_pytorch/myenv_jetson/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from config import get_config, get_weights_file_path\n",
    "from train import get_model, get_ds, run_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098d6eb",
   "metadata": {},
   "source": [
    "## Setup: Device and Data Loading\n",
    "\n",
    "Initialize the device (CUDA if available), load configuration, and prepare the data loaders and tokenizers for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2d1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Max length of source sentence: 466\n",
      "Max length of target sentence: 479\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bfdf99",
   "metadata": {},
   "source": [
    "## Load PyTorch Model\n",
    "\n",
    "Load the trained PyTorch transformer model from the checkpoint. The model is moved to the specified device (CUDA/CPU) and weights are loaded from the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd12198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = get_weights_file_path(config, f\"21\")\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "del state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34b7ae",
   "metadata": {},
   "source": [
    "## Prepare for Inference\n",
    "\n",
    "Set the model to evaluation mode and disable gradient computation for faster inference. Import libraries needed for benchmarking (time, numpy, pandas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edaeeaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bb38b",
   "metadata": {},
   "source": [
    "## Load TensorRT Model\n",
    "\n",
    "Load the split TensorRT engines (encoder, decoder, and projection layers). The memory fraction is limited to 60% to ensure TensorRT has enough GPU memory. The engines are loaded from the `tensorrt_split/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59c40dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRT Engines...\n",
      "Engines loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<run_trt_split.TRTTransformer at 0xfffed2e346d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: this import should NOT trigger inference because the file uses if __name__ == \"__main__\"\n",
    "from run_trt_split import TRTTransformer, greedy_decode as greedy_decode_trt\n",
    "\n",
    "# (optional) same memory fraction trick in notebook\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_per_process_memory_fraction(0.6)\n",
    "\n",
    "trt_model = TRTTransformer(\n",
    "    enc_path=\"tensorrt_split/tmodel_21_encoder_fp32.engine\",\n",
    "    dec_path=\"tensorrt_split/tmodel_21_decoder_fp32.engine\",\n",
    "    proj_path=\"tensorrt_split/tmodel_21_projection_fp32.engine\"\n",
    ")\n",
    "\n",
    "trt_model  # sanity: should print \"Engines loaded.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431ebf4",
   "metadata": {},
   "source": [
    "## Import Decoding Functions\n",
    "\n",
    "Import the greedy decoding function from the training module for PyTorch inference. The TensorRT version was already imported above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21fa68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import greedy_decode as greedy_decode_pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a9e79",
   "metadata": {},
   "source": [
    "## Benchmark Function\n",
    "\n",
    "The `benchmark_decode` function measures inference latency for a given model and decoding function. It:\n",
    "\n",
    "1. **Warmup Phase**: Runs a few inference passes to stabilize GPU performance and optionally prints sample translations\n",
    "2. **Timed Phase**: Measures actual inference time for multiple batches\n",
    "3. **Statistics**: Computes mean, median (p50), p90, and p99 percentiles of latency\n",
    "\n",
    "This function is used to compare PyTorch vs TensorRT performance.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a711112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_decode(\n",
    "    decode_fn,\n",
    "    model_obj,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"\",\n",
    "    print_warmup_samples=0 \n",
    "):\n",
    "    times_ms = []\n",
    "\n",
    "    # ---------------- Warmup (optional prints here) ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= warmup_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        out_ids = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "        if i < print_warmup_samples:\n",
    "            out_text = tokenizer_tgt.decode(out_ids.detach().cpu().numpy())\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{label} WARMUP SAMPLE {i+1}\")\n",
    "            print(f\"SOURCE:    {batch['src_text'][0]}\")\n",
    "            print(f\"TARGET:    {batch['tgt_text'][0]}\")\n",
    "            print(f\"PRED:      {out_text}\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # ---------------- Timed runs ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        _ = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        times_ms.append((t1 - t0) * 1000)\n",
    "\n",
    "    arr = np.array(times_ms, dtype=np.float32)\n",
    "    print(f\"\\n{label} latency over {len(arr)} batches:\")\n",
    "    print(f\"  mean: {arr.mean():.2f} ms\")\n",
    "    print(f\"  p50 : {np.percentile(arr, 50):.2f} ms\")\n",
    "    print(f\"  p90 : {np.percentile(arr, 90):.2f} ms\")\n",
    "    print(f\"  p99 : {np.percentile(arr, 99):.2f} ms\")\n",
    "\n",
    "    return times_ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d67ad",
   "metadata": {},
   "source": [
    "## Run Performance Benchmarks\n",
    "\n",
    "Benchmark both PyTorch and TensorRT models on the validation dataset. This will:\n",
    "\n",
    "- Run 5 warmup batches (with 2 sample translations printed)\n",
    "- Measure latency for 50 batches\n",
    "- Print latency statistics (mean, p50, p90, p99)\n",
    "- Calculate the speedup factor (PyTorch latency / TensorRT latency)\n",
    "\n",
    "The results show how much faster TensorRT is compared to PyTorch for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5eabe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 1\n",
      "SOURCE:    'Better – Yes, much better. – Wonderful! – It is not at all wonderful. Still, he's better!' they said in whispers, smiling at one another.\n",
      "TARGET:    »Geht es ihm besser?« – »Ja, bedeutend besser.« – »Wunderbar!« – »Dabei ist nichts Wunderbares.« – »Nun, jedenfalls geht es ihm besser«, sprachen sie flüsternd untereinander und lächelten einer dem andern zu.\n",
      "PRED:      » Nun , ja , dann ist es gut , es ist besser , wenn Sie nicht gut sind ; das ist ja doch immer !« sagte er , indem sie mit einem anderen Lächeln .\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 2\n",
      "SOURCE:    Then, in order to proceed \"by rule,\" the beadle conducted them right to the entrance near the square, where, pointing out with his cane a large circle of block-stones without inscription or carving—\n",
      "TARGET:    Programmgemäß führte sie der Schweizer nach dem Hauptportal zurück und zeigte ihnen mit seinem Stock einen großen Kreis von schwarzen Steinchen ohne irgendwelche Beigabe noch Inschrift.\n",
      "PRED:      Dann führte sie die Post , die der Kirchendiener an den Rand des , wo er mit seinen breiten des in , ohne » « oder ohne » « angebracht war .\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 3\n",
      "SOURCE:    A sort of undefined longing crept upon them. This took dim shape, presently--it was budding homesickness.\n",
      "TARGET:    Eine Art unbestimmter Sehnsucht ergriff sie und lastete immer schwerer auf ihnen -- es war das Heimweh.\n",
      "PRED:      Eine Art Sehnsucht ergriff sie , und zwar entfernte sie .\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 4\n",
      "SOURCE:    \"No.\"\n",
      "TARGET:    »Nein.«\n",
      "PRED:      » Nein .«\n",
      "\n",
      "PyTorch latency over 50 batches:\n",
      "  mean: 970.21 ms\n",
      "  p50 : 790.44 ms\n",
      "  p90 : 2121.20 ms\n",
      "  p99 : 2492.59 ms\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 1\n",
      "SOURCE:    But let me come to the point.\n",
      "TARGET:    Aber laß mich zum wichtigsten Punkt kommen.\n",
      "PRED:      Aber laß mich zum wichtigsten Punkt kommen .\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 2\n",
      "SOURCE:    They say it has caught up the rye.'\n",
      "TARGET:    Aber der ist von Roggen nicht zu unterscheiden.«\n",
      "PRED:      Sie haben ja den Arbeiter auf dem Lande getroffen .«\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 3\n",
      "SOURCE:    Master Jacques! let the spider work its will!\"\n",
      "TARGET:    Meister Jacob! lasset die Spinne gewähren!«\n",
      "PRED:      Meister Jacob ! laß die Spinne gewähren !«\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 4\n",
      "SOURCE:    Mr. Rochester told me to give you and Mary this.\"\n",
      "TARGET:    Mr. Rochester gab mir dies für Sie und Ihre Frau.«\n",
      "PRED:      Mr . Rochester sagte mir , daß ich Ihnen dies und Mary .«\n",
      "\n",
      "TensorRT latency over 50 batches:\n",
      "  mean: 884.26 ms\n",
      "  p50 : 433.95 ms\n",
      "  p90 : 991.32 ms\n",
      "  p99 : 10375.97 ms\n",
      "\n",
      "Speedup (PyTorch / TRT): 1.10x\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_len = config[\"seq_len\"]\n",
    "\n",
    "pt_times = benchmark_decode(\n",
    "    greedy_decode_pt, model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"PyTorch\",\n",
    "    print_warmup_samples=4\n",
    ")\n",
    "\n",
    "trt_times = benchmark_decode(\n",
    "    greedy_decode_trt, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"TensorRT\",\n",
    "    print_warmup_samples=4\n",
    ")\n",
    "\n",
    "speedup = np.mean(pt_times) / np.mean(trt_times)\n",
    "print(f\"\\nSpeedup (PyTorch / TRT): {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f0bc4",
   "metadata": {},
   "source": [
    "## Translation Comparison\n",
    "\n",
    "The `verbose_compare_same_sample` function compares translations from both models on the same input sample. It:\n",
    "\n",
    "- Takes the same batch from the dataloader\n",
    "- Runs inference with both PyTorch and TensorRT models\n",
    "- Prints side-by-side comparison of source, target, and predictions\n",
    "- Returns the token IDs from both models\n",
    "\n",
    "This helps verify that both models produce similar (or identical) translations.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8940670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE:     The grand poem, the grand edifice, the grand work of humanity will no longer be built: it will be printed.\n",
      "TARGET:     Die große Dichtung, das große Denkmal, das große Kunstwerk der Menschheit wird nicht mehr gebaut, es wird gedruckt werden.\n",
      "PT  PRED:   Das große , das große Gebäude , das großen der Menschheit wird nicht mehr ; es wird die heilige Ordnung verderben .\n",
      "TRT PRED:   Das große , das große Gebäude , das großen der Menschheit wird nicht mehr ; es wird die heilige Ordnung verderben .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def verbose_compare_same_sample(\n",
    "    pt_model,\n",
    "    trt_model,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    batch_index=0,   # pick which batch to compare\n",
    "):\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---- get ONE specific batch ----\n",
    "    it = iter(dataloader)\n",
    "    batch = None\n",
    "    for i in range(batch_index + 1):\n",
    "        batch = next(it)\n",
    "\n",
    "    encoder_input = batch[\"encoder_input\"].to(device)\n",
    "    encoder_mask  = batch[\"encoder_mask\"].to(device)\n",
    "    source_text   = batch[\"src_text\"][0]\n",
    "    target_text   = batch[\"tgt_text\"][0]\n",
    "\n",
    "    # ---- PyTorch decode on SAME tensors ----\n",
    "    out_pt_ids = greedy_decode_pt(\n",
    "        pt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_pt_text = tokenizer_tgt.decode(out_pt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- TensorRT decode on SAME tensors ----\n",
    "    out_trt_ids = greedy_decode_trt(\n",
    "        trt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_trt_text = tokenizer_tgt.decode(out_trt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- print SAME sample ----\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"SOURCE:     {source_text}\")\n",
    "    print(f\"TARGET:     {target_text}\")\n",
    "    print(f\"PT  PRED:   {out_pt_text}\")\n",
    "    print(f\"TRT PRED:   {out_trt_text}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    return out_pt_ids, out_trt_ids\n",
    "\n",
    "\n",
    "# run on batch 0 (same sample)\n",
    "_ = verbose_compare_same_sample(\n",
    "    model, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    config[\"seq_len\"], device,\n",
    "    batch_index=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97e13c",
   "metadata": {},
   "source": [
    "## Logits Comparison (First Decoding Step)\n",
    "\n",
    "The `compare_first_step_logits` function verifies numerical accuracy by comparing the logits from the first decoding step. It:\n",
    "\n",
    "- Encodes the source sequence with both models\n",
    "- Runs one decoder step (with [SOS] token) for both models\n",
    "- Compares the output logits (vocabulary probabilities)\n",
    "- Reports maximum and mean absolute differences\n",
    "\n",
    "Small differences (< 0.01) indicate that TensorRT is producing numerically similar outputs to PyTorch, which is expected due to floating-point precision differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6fdf7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-step logits diff:\n",
      "  max abs: 0.007763862609863281\n",
      "  mean abs: 0.0011671466054394841\n"
     ]
    }
   ],
   "source": [
    "from dataset import causal_mask\n",
    "def compare_first_step_logits(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    # PT: encoder + one decoder step (SOS)\n",
    "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "    dec_in = torch.tensor([[sos_idx]], device=device, dtype=src.dtype)\n",
    "\n",
    "    enc_pt = pt_model.encode(src, src_mask)\n",
    "    dec_mask = causal_mask(1).type_as(src_mask).to(device)  # (1,1,1)\n",
    "    out_pt = pt_model.decode(enc_pt, src_mask, dec_in, dec_mask)\n",
    "    logits_pt = pt_model.project(out_pt[:, -1])  # (B, vocab)\n",
    "\n",
    "    # TRT: same\n",
    "    enc_trt = trt_model.encode(src, src_mask)\n",
    "    dec_mask_trt = causal_mask(1).type_as(src_mask).to(device).unsqueeze(1)  # (1,1,1,1)\n",
    "    out_trt = trt_model.decode(enc_trt, src_mask, dec_in, dec_mask_trt)\n",
    "    logits_trt = trt_model.project(out_trt[:, -1])\n",
    "\n",
    "    lp = logits_pt.detach().cpu().float()\n",
    "    lt = logits_trt.detach().cpu().float()\n",
    "\n",
    "    max_abs = (lp - lt).abs().max().item()\n",
    "    mean_abs = (lp - lt).abs().mean().item()\n",
    "    print(\"First-step logits diff:\")\n",
    "    print(\"  max abs:\", max_abs)\n",
    "    print(\"  mean abs:\", mean_abs)\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_first_step_logits(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0b996",
   "metadata": {},
   "source": [
    "## Encoder Output Comparison\n",
    "\n",
    "The `compare_encoder_outputs` function verifies that the encoder produces similar outputs in both models. It:\n",
    "\n",
    "- **`pt_encode_trt_style`**: Adapts PyTorch encoder to use the same mask format as TensorRT (square mask shape)\n",
    "- **`compare_encoder_outputs`**: Compares encoder outputs from both models and reports maximum and mean absolute differences\n",
    "\n",
    "This ensures that the encoder component is working correctly in the TensorRT version. Differences should be very small (< 0.001) due to floating-point precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c220954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER diff: max = 0.0009958148002624512 mean = 5.0932674639625475e-05\n"
     ]
    }
   ],
   "source": [
    "def pt_encode_trt_style(pt_model, src, src_mask):\n",
    "    # Make PT use the same square mask shape TRT encoder expects\n",
    "    S = src.shape[1]\n",
    "    if src_mask.dim() == 3:\n",
    "        src_mask = src_mask.unsqueeze(1)   # (B,1,S)->(B,1,1,S)\n",
    "    if src_mask.dim() == 4 and src_mask.shape[2] == 1:\n",
    "        src_mask = src_mask.repeat(1,1,S,1)  # -> (B,1,S,S)\n",
    "    src_mask = src_mask.float()  # binary float\n",
    "    return pt_model.encode(src, src_mask)\n",
    "\n",
    "def compare_encoder_outputs(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    enc_pt  = pt_encode_trt_style(pt_model, src, src_mask).detach().cpu().float()\n",
    "    enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "    diff = (enc_pt - enc_trt).abs()\n",
    "    print(\"ENCODER diff: max =\", diff.max().item(), \"mean =\", diff.mean().item())\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_encoder_outputs(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8351e3",
   "metadata": {},
   "source": [
    "## Position-wise Encoder Analysis\n",
    "\n",
    "This cell performs a detailed analysis of encoder output differences:\n",
    "\n",
    "- Computes per-position mean differences across the hidden dimension\n",
    "- Separates differences for **padded positions** (where mask = 0) vs **unpadded positions** (where mask = 1)\n",
    "- Reports mean differences for each category and the overall maximum\n",
    "\n",
    "This helps understand if differences are concentrated in padded regions (which are typically ignored) or in actual content positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96bb3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean diff padded   : 6.945923814782873e-05\n",
      "mean diff unpadded : 3.1320138077717274e-05\n",
      "max diff overall   : 0.00021883004228584468\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "batch0 = next(iter(val_dataloader))\n",
    "src = batch0[\"encoder_input\"].to(device)\n",
    "src_mask = batch0[\"encoder_mask\"].to(device)\n",
    "\n",
    "enc_pt  = pt_encode_trt_style(model, src, src_mask).detach().cpu().float()\n",
    "enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "# per-position mean diff across hidden dim\n",
    "pos_diff = (enc_pt - enc_trt).abs().mean(-1).squeeze(0)   # (S,)\n",
    "\n",
    "# padded vs unpadded positions (mask is (1,1,1,S))\n",
    "key_mask = src_mask.squeeze().cpu()  # (S,) with 0/1\n",
    "pad_pos = key_mask == 0\n",
    "unpad_pos = key_mask == 1\n",
    "\n",
    "print(\"mean diff padded   :\", pos_diff[pad_pos].mean().item())\n",
    "print(\"mean diff unpadded :\", pos_diff[unpad_pos].mean().item())\n",
    "print(\"max diff overall   :\", pos_diff.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a98e9",
   "metadata": {},
   "source": [
    "## Export Benchmark Results\n",
    "\n",
    "Save the benchmark timing data to a CSV file for further analysis. The DataFrame contains:\n",
    "\n",
    "- `pytorch_ms`: Latency for each PyTorch inference (milliseconds)\n",
    "- `tensorrt_ms`: Latency for each TensorRT inference (milliseconds)\n",
    "- `speedup_x`: Per-sample speedup ratio (PyTorch / TensorRT)\n",
    "\n",
    "The `describe()` method provides summary statistics (mean, std, min, max, percentiles) for all columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3569b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pytorch_ms</th>\n",
       "      <th>tensorrt_ms</th>\n",
       "      <th>speedup_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>970.210403</td>\n",
       "      <td>884.263905</td>\n",
       "      <td>2.824022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>654.205220</td>\n",
       "      <td>1994.812289</td>\n",
       "      <td>2.894434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>147.489438</td>\n",
       "      <td>53.184236</td>\n",
       "      <td>0.082573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>469.302389</td>\n",
       "      <td>229.167503</td>\n",
       "      <td>1.077197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>790.436201</td>\n",
       "      <td>433.947643</td>\n",
       "      <td>1.948705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1351.405550</td>\n",
       "      <td>702.016890</td>\n",
       "      <td>3.707975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2666.658738</td>\n",
       "      <td>11598.590235</td>\n",
       "      <td>14.585807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pytorch_ms   tensorrt_ms  speedup_x\n",
       "count    50.000000     50.000000  50.000000\n",
       "mean    970.210403    884.263905   2.824022\n",
       "std     654.205220   1994.812289   2.894434\n",
       "min     147.489438     53.184236   0.082573\n",
       "25%     469.302389    229.167503   1.077197\n",
       "50%     790.436201    433.947643   1.948705\n",
       "75%    1351.405550    702.016890   3.707975\n",
       "max    2666.658738  11598.590235  14.585807"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = min(len(pt_times), len(trt_times))\n",
    "df = pd.DataFrame({\n",
    "    \"pytorch_ms\": pt_times[:m],\n",
    "    \"tensorrt_ms\": trt_times[:m],\n",
    "})\n",
    "df[\"speedup_x\"] = df[\"pytorch_ms\"] / df[\"tensorrt_ms\"]\n",
    "\n",
    "df.to_csv(\"benchmark_times.csv\", index=False)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36673b4b",
   "metadata": {},
   "source": [
    "## Optional: Full Validation\n",
    "\n",
    "Uncomment the line below to run full validation on the PyTorch model. This will print multiple translation examples with source, target, and predicted outputs. This is useful for qualitative assessment of translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31231d48",
   "metadata": {},
   "source": [
    "# Checking the tensorboard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are on ssh then make sure you are doing port forwarding\n",
    "#ssh -L 6005:localhost:6005 user@jetson_ip\n",
    "#Finally on your host on a browser open http://localhost:6005\n",
    "\n",
    "logdir = \"./runs/tmodel/\"\n",
    "os.system(f\"tensorboard --logdir {logdir} --port 6005\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_jetson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
