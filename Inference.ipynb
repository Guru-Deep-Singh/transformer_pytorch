{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f67ede",
   "metadata": {},
   "source": [
    "# Running Inference\n",
    "\n",
    "This notebook demonstrates how to run inference with both PyTorch and TensorRT models, benchmark their performance, and verify that both models produce similar outputs. It includes:\n",
    "\n",
    "- Loading PyTorch and TensorRT models\n",
    "- Performance benchmarking (latency comparison)\n",
    "- Output verification (comparing translations and numerical outputs)\n",
    "- Encoder output accuracy verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac52c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guru/git_reps/transformer_pytorch/myenv_jetson/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from config import get_config, get_weights_file_path\n",
    "from train import get_model, get_ds, run_validation\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce0978",
   "metadata": {},
   "source": [
    "## Let's find the GPU specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d018d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: 1\n",
      "\n",
      "---------------------------\n",
      "\n",
      "All Properties: _CudaDeviceProperties(name='Orin', major=8, minor=7, total_memory=7619MB, multi_processor_count=8, uuid=6c7f4d62-8e83-589a-a8af-556d15b4a582, pci_bus_id=0, pci_device_id=0, pci_domain_id=0, L2_cache_size=2MB)\n",
      "---------------------------\n",
      "\n",
      "--- GPU Index: 0 ---\n",
      "Name: Orin\n",
      "Processor Count: 8\n",
      "Total Memory: 7.44 GB\n",
      "Warp Size: 32\n",
      "Memory Clock Rate: N/A Hz\n",
      "Memory Bus Width: N/A bits\n",
      "Gcn Arch: N/A\n",
      "Compute Capability (arch): 8.7\n",
      "Is Integrated: 1\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_info():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No GPU detected by PyTorch.\")\n",
    "        return\n",
    "    \n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"GPUs detected: {num_devices}\\n\")\n",
    "\n",
    "    for device_idx in range(num_devices):\n",
    "        device_name = torch.cuda.get_device_name(device_idx)\n",
    "        props = torch.cuda.get_device_properties(device_idx)\n",
    "        print(\"---------------------------\\n\")\n",
    "        print(f\"All Properties: {props}\")\n",
    "        print(\"---------------------------\\n\")\n",
    "\n",
    "        print(f\"--- GPU Index: {device_idx} ---\")\n",
    "        print(f\"Name: {device_name}\")\n",
    "        print(f\"Processor Count: {props.multi_processor_count}\")\n",
    "        print(f\"Total Memory: {props.total_memory / (1024 ** 3):.2f} GB\")\n",
    "        #print(f\"Max Threads per Block: {props.max_threads_per_block}\")\n",
    "        print(f\"Warp Size: {props.warp_size}\")\n",
    "        #print(f\"Clock Rate: {props.clock_rate / 1e6:.2f} GHz\")\n",
    "        print(f\"Memory Clock Rate: {getattr(props, 'memory_clock_rate', 'N/A')} Hz\")\n",
    "        print(f\"Memory Bus Width: {getattr(props, 'memory_bus_width', 'N/A')} bits\")\n",
    "        print(f\"Gcn Arch: {getattr(props, 'gcnArch', 'N/A')}\")\n",
    "        print(f\"Compute Capability (arch): \"\n",
    "              f\"{getattr(props, 'major', 'N/A')}.{getattr(props, 'minor', 'N/A')}\")\n",
    "        print(f\"Is Integrated: {props.is_integrated}\")\n",
    "        print(\"---------------------------\\n\")\n",
    "\n",
    "get_gpu_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098d6eb",
   "metadata": {},
   "source": [
    "## Setup: Device and Data Loading\n",
    "\n",
    "Initialize the device (CUDA if available), load configuration, and prepare the data loaders and tokenizers for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2d1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "The dataset en-de is not available.\n",
      "Checking availability of de-en...\n",
      "Loaded dataset with config: de-en\n",
      "Max length of source sentence: 466\n",
      "Max length of target sentence: 479\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05cec883",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = \"29\"\n",
    "PRECISION = \"fp16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bfdf99",
   "metadata": {},
   "source": [
    "## Load PyTorch Model\n",
    "\n",
    "Load the trained PyTorch transformer model from the checkpoint. The model is moved to the specified device (CUDA/CPU) and weights are loaded from the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12198b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# When I trained a model I used custom LayerNormalization which had a problem with fp16\n",
    "# I replaced it with nn.LayerNorm thus need to have a patch to map weights with the trained values\n",
    "PATCH = False\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = get_weights_file_path(config, EPOCH)\n",
    "checkpoint  = torch.load(model_filename,  map_location=\"cpu\")\n",
    "state_dict = checkpoint[\"model_state_dict\"]\n",
    "\n",
    "if PATCH:\n",
    "    # Rename 'alpha' to 'weight'\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.endswith(\".alpha\"):\n",
    "            # Replace .alpha with .weight\n",
    "            new_key = key.replace(\".alpha\", \".weight\")\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    del checkpoint, state_dict, new_state_dict\n",
    "else:\n",
    "    model.load_state_dict(state_dict)\n",
    "    del checkpoint, state_dict\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd41343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 85137712\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34b7ae",
   "metadata": {},
   "source": [
    "## Prepare for Inference\n",
    "\n",
    "Set the model to evaluation mode and disable gradient computation for faster inference. Import libraries needed for benchmarking (time, numpy, pandas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edaeeaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bb38b",
   "metadata": {},
   "source": [
    "## Load TensorRT Model\n",
    "\n",
    "Load the split TensorRT engines (encoder, decoder, and projection layers). The memory fraction is limited to 60% to ensure TensorRT has enough GPU memory. The engines are loaded from the `tensorrt_split/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c40dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRT Engines...\n",
      "Engines loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<run_trt_split.TRTTransformer at 0xfffefe77e080>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: this import should NOT trigger inference because the file uses if __name__ == \"__main__\"\n",
    "from run_trt_split import TRTTransformer, greedy_decode as greedy_decode_trt\n",
    "\n",
    "# (optional) same memory fraction trick in notebook\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_per_process_memory_fraction(0.6)\n",
    "\n",
    "trt_model = TRTTransformer(\n",
    "    enc_path=\"tensorrt_split/tmodel_\" + EPOCH + \"_encoder_\" + PRECISION + \".engine\",\n",
    "    dec_path=\"tensorrt_split/tmodel_\" + EPOCH + \"_decoder_\" + PRECISION + \".engine\",\n",
    "    proj_path=\"tensorrt_split/tmodel_\" + EPOCH + \"_projection_\" + PRECISION + \".engine\"\n",
    ")\n",
    "\n",
    "trt_model  # sanity: should print \"Engines loaded.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e0d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRECISION == \"fp16\":\n",
    "    model = model.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431ebf4",
   "metadata": {},
   "source": [
    "## Import Decoding Functions\n",
    "\n",
    "Import the greedy decoding function from the training module for PyTorch inference. The TensorRT version was already imported above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21fa68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import greedy_decode as greedy_decode_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a9e79",
   "metadata": {},
   "source": [
    "## Benchmark Function\n",
    "\n",
    "The `benchmark_decode` function measures inference latency for a given model and decoding function. It:\n",
    "\n",
    "1. **Warmup Phase**: Runs a few inference passes to stabilize GPU performance and optionally prints sample translations\n",
    "2. **Timed Phase**: Measures actual inference time for multiple batches\n",
    "3. **Statistics**: Computes mean, median (p50), p90, and p99 percentiles of latency\n",
    "\n",
    "This function is used to compare PyTorch vs TensorRT performance.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a711112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_decode(\n",
    "    decode_fn,\n",
    "    model_obj,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"\",\n",
    "    print_warmup_samples=0 \n",
    "):\n",
    "    times_ms = []\n",
    "\n",
    "    # ---------------- Warmup (optional prints here) ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= warmup_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        out_ids = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "        if i < print_warmup_samples:\n",
    "            out_text = tokenizer_tgt.decode(out_ids.detach().cpu().numpy())\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{label} WARMUP SAMPLE {i+1}\")\n",
    "            print(f\"SOURCE:    {batch['src_text'][0]}\")\n",
    "            print(f\"TARGET:    {batch['tgt_text'][0]}\")\n",
    "            print(f\"PRED:      {out_text}\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # ---------------- Timed runs ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        _ = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        times_ms.append((t1 - t0) * 1000)\n",
    "\n",
    "    arr = np.array(times_ms, dtype=np.float32)\n",
    "    print(f\"\\n{label} latency over {len(arr)} batches:\")\n",
    "    print(f\"  mean: {arr.mean():.2f} ms\")\n",
    "    print(f\"  p50 : {np.percentile(arr, 50):.2f} ms\")\n",
    "    print(f\"  p90 : {np.percentile(arr, 90):.2f} ms\")\n",
    "    print(f\"  p99 : {np.percentile(arr, 99):.2f} ms\")\n",
    "\n",
    "    return times_ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d67ad",
   "metadata": {},
   "source": [
    "## Run Performance Benchmarks\n",
    "\n",
    "Benchmark both PyTorch and TensorRT models on the validation dataset. This will:\n",
    "\n",
    "- Run 5 warmup batches (with 2 sample translations printed)\n",
    "- Measure latency for 50 batches\n",
    "- Print latency statistics (mean, p50, p90, p99)\n",
    "- Calculate the speedup factor (PyTorch latency / TensorRT latency)\n",
    "\n",
    "The results show how much faster TensorRT is compared to PyTorch for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5eabe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 1\n",
      "SOURCE:    That is where technique comes in.'\n",
      "TARGET:    Da sieht man den Wert der Technik!«\n",
      "PRED:      Da sieht man den Tod ein .«\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 2\n",
      "SOURCE:    And Alice was so much frightened that she ran off at once in the direction it pointed to, without trying to explain the mistake it had made.\n",
      "TARGET:    Schnell, vorwärts!« Alice war so erschrocken, daß sie schnell in der angedeuteten Richtung fortlief, ohne ihm zu erklären, daß es sich versehen habe.\n",
      "PRED:      Und Alice war so erschrocken , daß sie schnell in die Höhe stürzte , sich ohne zu erklären , daß es sich nur nicht erklären könne , den Irrtum begangen hatte .\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 3\n",
      "SOURCE:    It was a curious laugh; distinct, formal, mirthless.\n",
      "TARGET:    Es war ein seltsames Lachen, deutlich, förmlich, freudlos.\n",
      "PRED:      Es war ein seltsames Lachen , deutlich , förmlich , kurz und wann .\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 4\n",
      "SOURCE:    'There is one thing I want to say...' the Princess began, and from her serious look Kitty guessed what was coming.\n",
      "TARGET:    »Ich möchte dir nur das eine sagen ...«, begann die Fürstin, und an ihrem ernsten, eine lebhafte Erregung bekundenden Gesichte erriet Kitty, wovon die Rede sein sollte.\n",
      "PRED:      » Ich will nur sagen , ich will nur sagen ...« begann die Fürstin , und Kitty merkte an dem ernsten , ernsten Blicke , das kam ihr nicht recht zum Ausdruck .\n",
      "\n",
      "PyTorch latency over 50 batches:\n",
      "  mean: 597.53 ms\n",
      "  p50 : 534.40 ms\n",
      "  p90 : 1077.49 ms\n",
      "  p99 : 1734.12 ms\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 1\n",
      "SOURCE:    But in the doorkeeper's character there are also other features which might be very useful for those who seek entry to the law, and when he hinted at some possibility in the future it always seemed to make it clear that he might even go beyond his duty.\n",
      "TARGET:    Nun mischen sich aber in den Türhüter noch andere Wesenszüge ein, die für den, der Einlaß verlangt, sehr günstig sind und welche es immerhin begreiflich machen, daß er in jener Andeutung einer zukünftigen Möglichkeit über seine Pflicht etwas hinausgehen konnte.\n",
      "PRED:      Aber in der Türhüter stellt sich auch andere Züge , die für den , wie sie es sind , sehr nützlich zu sein , und wenn er sich selbst keine Möglichkeit dazu verstehen , so würden sie es immer so deutlich machen , daß er sogar in Freiheit geht , seine Pflicht zu gehen .\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 2\n",
      "SOURCE:    The judge, by the way, was the King; and as he wore his crown over the wig, (look at the frontispiece if you want to see how he did it,) he did not look at all comfortable, and it was certainly not becoming.\n",
      "TARGET:    Der Richter war übrigens der König, und er trug die Krone über der Perücke (seht euch das Titelbild an, wenn ihr wissen wollt, wie), es sah nicht aus, als sei es ihm bequem, und sicherlich stand es ihm nicht gut.\n",
      "PRED:      Der Richter war übrigens der König , und er trug die Krone über der Perücke ( seht euch das an , wenn ihr das nicht gefiel , und er blickte nicht so gut an .\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 3\n",
      "SOURCE:    And there was a last \"adieu\" divided into two words!\n",
      "TARGET:    Er setzte noch ein »A dieu!« darunter, in zwei Worten geschrieben.\n",
      "PRED:      Er setzte noch ein » A !« darunter , in zwei Worten geschrieben .\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 4\n",
      "SOURCE:    He's generally drunk enough.\"\n",
      "TARGET:    Der ist ohnehin immer betrunken genug!\"\n",
      "PRED:      Der ist ohnehin immer betrunken genug !\"\n",
      "\n",
      "TensorRT latency over 50 batches:\n",
      "  mean: 362.49 ms\n",
      "  p50 : 300.51 ms\n",
      "  p90 : 705.51 ms\n",
      "  p99 : 970.46 ms\n",
      "\n",
      "Speedup (PyTorch / TRT): 1.65x\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_len = config[\"seq_len\"]\n",
    "\n",
    "pt_times = benchmark_decode(\n",
    "    greedy_decode_pt, model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"PyTorch\",\n",
    "    print_warmup_samples=4\n",
    ")\n",
    "\n",
    "trt_times = benchmark_decode(\n",
    "    greedy_decode_trt, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"TensorRT\",\n",
    "    print_warmup_samples=4\n",
    ")\n",
    "\n",
    "speedup = np.mean(pt_times) / np.mean(trt_times)\n",
    "print(f\"\\nSpeedup (PyTorch / TRT): {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962067f",
   "metadata": {},
   "source": [
    "| GPU        | Runtime Screenshot | GPU Details Screenshot |\n",
    "|------------|------------------|----------------------|\n",
    "| **MI300X** | ![MI300X Runtime](./images/rocm_runtime_pytorch.png) | ![MI300X Details](./images/amd_gpu_details.png) |\n",
    "| **Tesla T4**     | ![T4 Runtime](./images/tesla_t4_runtime.png) | ![T4 Details](./images/tesla_t4_details.png) |\n",
    "| **L4**     | ![L4 Runtime](./images/l4_runtime.png) | ![L4 Details](./images/l4_details.png) |\n",
    "| **A100**   | ![A100 Runtime](./images/a100_runtime.png) | ![A100 Details](./images/a100_details.png) |\n",
    "| **Jetson Orin Nano Super**   | ![Jetson Runtime](./images/jetson_runtime.png) | ![Jetson Details](./images/jetson_details.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f0bc4",
   "metadata": {},
   "source": [
    "## Translation Comparison\n",
    "\n",
    "The `verbose_compare_same_sample` function compares translations from both models on the same input sample. It:\n",
    "\n",
    "- Takes the same batch from the dataloader\n",
    "- Runs inference with both PyTorch and TensorRT models\n",
    "- Prints side-by-side comparison of source, target, and predictions\n",
    "- Returns the token IDs from both models\n",
    "\n",
    "This helps verify that both models produce similar (or identical) translations.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8940670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE:     'I am very glad you have come.\n",
      "TARGET:     »Ich freue mich sehr, daß du hergekommen bist.\n",
      "PT  PRED:   » Ich freue mich sehr , daß du hergekommen bist .\n",
      "TRT PRED:   » Ich freue mich sehr , daß du hergekommen bist .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def verbose_compare_same_sample(\n",
    "    pt_model,\n",
    "    trt_model,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    batch_index=0,   # pick which batch to compare\n",
    "):\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---- get ONE specific batch ----\n",
    "    it = iter(dataloader)\n",
    "    batch = None\n",
    "    for i in range(batch_index + 1):\n",
    "        batch = next(it)\n",
    "\n",
    "    encoder_input = batch[\"encoder_input\"].to(device)\n",
    "    encoder_mask  = batch[\"encoder_mask\"].to(device)\n",
    "    source_text   = batch[\"src_text\"][0]\n",
    "    target_text   = batch[\"tgt_text\"][0]\n",
    "\n",
    "    # ---- PyTorch decode on SAME tensors ----\n",
    "    out_pt_ids = greedy_decode_pt(\n",
    "        pt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_pt_text = tokenizer_tgt.decode(out_pt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- TensorRT decode on SAME tensors ----\n",
    "    out_trt_ids = greedy_decode_trt(\n",
    "        trt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_trt_text = tokenizer_tgt.decode(out_trt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- print SAME sample ----\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"SOURCE:     {source_text}\")\n",
    "    print(f\"TARGET:     {target_text}\")\n",
    "    print(f\"PT  PRED:   {out_pt_text}\")\n",
    "    print(f\"TRT PRED:   {out_trt_text}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    return out_pt_ids, out_trt_ids\n",
    "\n",
    "\n",
    "# run on batch 0 (same sample)\n",
    "_ = verbose_compare_same_sample(\n",
    "    model, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    config[\"seq_len\"], device,\n",
    "    batch_index=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97e13c",
   "metadata": {},
   "source": [
    "## Logits Comparison (First Decoding Step)\n",
    "\n",
    "The `compare_first_step_logits` function verifies numerical accuracy by comparing the logits from the first decoding step. It:\n",
    "\n",
    "- Encodes the source sequence with both models\n",
    "- Runs one decoder step (with [SOS] token) for both models\n",
    "- Compares the output logits (vocabulary probabilities)\n",
    "- Reports maximum and mean absolute differences\n",
    "\n",
    "Small differences (< 0.01) indicate that TensorRT is producing numerically similar outputs to PyTorch, which is expected due to floating-point precision differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6fdf7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-step logits diff:\n",
      "  max abs: 0.0380859375\n",
      "  mean abs: 0.01216646283864975\n"
     ]
    }
   ],
   "source": [
    "from dataset import causal_mask\n",
    "def compare_first_step_logits(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    # PT: encoder + one decoder step (SOS)\n",
    "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "    dec_in = torch.tensor([[sos_idx]], device=device, dtype=src.dtype)\n",
    "\n",
    "    enc_pt = pt_model.encode(src, src_mask)\n",
    "    dec_mask = causal_mask(1).type_as(src_mask).to(device)  # (1,1,1)\n",
    "    out_pt = pt_model.decode(enc_pt, src_mask, dec_in, dec_mask)\n",
    "    logits_pt = pt_model.project(out_pt[:, -1])  # (B, vocab)\n",
    "\n",
    "    # TRT: same\n",
    "    enc_trt = trt_model.encode(src, src_mask)\n",
    "    dec_mask_trt = causal_mask(1).type_as(src_mask).to(device).unsqueeze(1)  # (1,1,1,1)\n",
    "    out_trt = trt_model.decode(enc_trt, src_mask, dec_in, dec_mask_trt)\n",
    "    logits_trt = trt_model.project(out_trt[:, -1])\n",
    "\n",
    "    lp = logits_pt.detach().cpu().float()\n",
    "    lt = logits_trt.detach().cpu().float()\n",
    "\n",
    "    max_abs = (lp - lt).abs().max().item()\n",
    "    mean_abs = (lp - lt).abs().mean().item()\n",
    "    print(\"First-step logits diff:\")\n",
    "    print(\"  max abs:\", max_abs)\n",
    "    print(\"  mean abs:\", mean_abs)\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_first_step_logits(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0b996",
   "metadata": {},
   "source": [
    "## Encoder Output Comparison\n",
    "\n",
    "The `compare_encoder_outputs` function verifies that the encoder produces similar outputs in both models. It:\n",
    "\n",
    "- **`pt_encode_trt_style`**: Adapts PyTorch encoder to use the same mask format as TensorRT (square mask shape)\n",
    "- **`compare_encoder_outputs`**: Compares encoder outputs from both models and reports maximum and mean absolute differences\n",
    "\n",
    "This ensures that the encoder component is working correctly in the TensorRT version. Differences should be very small (< 0.001) due to floating-point precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c220954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER diff: max = 0.003599703311920166 mean = 0.00021042623848188668\n"
     ]
    }
   ],
   "source": [
    "def pt_encode_trt_style(pt_model, src, src_mask):\n",
    "    # Make PT use the same square mask shape TRT encoder expects\n",
    "    S = src.shape[1]\n",
    "    if src_mask.dim() == 3:\n",
    "        src_mask = src_mask.unsqueeze(1)   # (B,1,S)->(B,1,1,S)\n",
    "    if src_mask.dim() == 4 and src_mask.shape[2] == 1:\n",
    "        src_mask = src_mask.repeat(1,1,S,1)  # -> (B,1,S,S)\n",
    "    src_mask = src_mask.float()  # binary float\n",
    "    return pt_model.encode(src, src_mask)\n",
    "\n",
    "def compare_encoder_outputs(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    enc_pt  = pt_encode_trt_style(pt_model, src, src_mask).detach().cpu().float()\n",
    "    enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "    diff = (enc_pt - enc_trt).abs()\n",
    "    print(\"ENCODER diff: max =\", diff.max().item(), \"mean =\", diff.mean().item())\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_encoder_outputs(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8351e3",
   "metadata": {},
   "source": [
    "## Position-wise Encoder Analysis\n",
    "\n",
    "This cell performs a detailed analysis of encoder output differences:\n",
    "\n",
    "- Computes per-position mean differences across the hidden dimension\n",
    "- Separates differences for **padded positions** (where mask = 0) vs **unpadded positions** (where mask = 1)\n",
    "- Reports mean differences for each category and the overall maximum\n",
    "\n",
    "This helps understand if differences are concentrated in padded regions (which are typically ignored) or in actual content positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96bb3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean diff padded   : 0.00021706544794142246\n",
      "mean diff unpadded : 0.0001546969433547929\n",
      "max diff overall   : 0.000515965570230037\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "batch0 = next(iter(val_dataloader))\n",
    "src = batch0[\"encoder_input\"].to(device)\n",
    "src_mask = batch0[\"encoder_mask\"].to(device)\n",
    "\n",
    "enc_pt  = pt_encode_trt_style(model, src, src_mask).detach().cpu().float()\n",
    "enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "# per-position mean diff across hidden dim\n",
    "pos_diff = (enc_pt - enc_trt).abs().mean(-1).squeeze(0)   # (S,)\n",
    "\n",
    "# padded vs unpadded positions (mask is (1,1,1,S))\n",
    "key_mask = src_mask.squeeze().cpu()  # (S,) with 0/1\n",
    "pad_pos = key_mask == 0\n",
    "unpad_pos = key_mask == 1\n",
    "\n",
    "print(\"mean diff padded   :\", pos_diff[pad_pos].mean().item())\n",
    "print(\"mean diff unpadded :\", pos_diff[unpad_pos].mean().item())\n",
    "print(\"max diff overall   :\", pos_diff.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a98e9",
   "metadata": {},
   "source": [
    "## Export Benchmark Results\n",
    "\n",
    "Save the benchmark timing data to a CSV file for further analysis. The DataFrame contains:\n",
    "\n",
    "- `pytorch_ms`: Latency for each PyTorch inference (milliseconds)\n",
    "- `tensorrt_ms`: Latency for each TensorRT inference (milliseconds)\n",
    "- `speedup_x`: Per-sample speedup ratio (PyTorch / TensorRT)\n",
    "\n",
    "The `describe()` method provides summary statistics (mean, std, min, max, percentiles) for all columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3569b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = min(len(pt_times), len(trt_times))\n",
    "df = pd.DataFrame({\n",
    "    \"pytorch_ms\": pt_times[:m],\n",
    "    \"tensorrt_ms\": trt_times[:m],\n",
    "})\n",
    "df[\"speedup_x\"] = df[\"pytorch_ms\"] / df[\"tensorrt_ms\"]\n",
    "\n",
    "df.to_csv(\"benchmark_times.csv\", index=False)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36673b4b",
   "metadata": {},
   "source": [
    "## Optional: Full Validation\n",
    "\n",
    "Uncomment the line below to run full validation on the PyTorch model. This will print multiple translation examples with source, target, and predicted outputs. This is useful for qualitative assessment of translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b72c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Good Virgin! good Virgin of heaven! my infant Jesus has been taken from me, has been stolen from me; they devoured her on a heath, they drank her blood, they cracked her bones!\n",
      "    TARGET: Süße Jungfrau! süße Jungfrau im Himmel droben! mein eigenes Jesuskind hat man mir genommen, hat man mir gestohlen, hat man auf einer Haide gefressen; man hat sein Blut getrunken, seine Gebeine zermalmt!\n",
      " PREDICTED: Heilige Jungfrau ! süße Jungfrau im Himmel ! mein Kind ist mir genommen worden , und sie hat mir einen Vicomte gestohlen , so haben sie auch sie ihr Blut getrunken .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And it seemed to Gregor much more sensible to leave him now in peace instead of disturbing him with talking at him and crying.\n",
      "    TARGET: Und Gregor schien es, daß es viel vernünftiger wäre, ihn jetzt in Ruhe zu lassen, statt ihn mit Weinen und Zureden zu stören.\n",
      " PREDICTED: Und Gregor schien es , daß Gregor jetzt viel gefährlicher wäre , und statt mit ihm zu plaudern und zu weinen .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: If he were insane, however, his was a very cool and collected insanity: I had never seen that handsome-featured face of his look more like chiselled marble than it did just now, as he put aside his snow-wet hair from his forehead and let the firelight shine free on his pale brow and cheek as pale, where it grieved me to discover the hollow trace of care or sorrow now so plainly graved.\n",
      "    TARGET: Indessen, wenn er wahnsinnig, so war sein Wahnsinn ein sehr stiller und harmloser; niemals hatte ich sein schönes, gemeißeltes Gesicht marmorähnlicher aussehend gefunden, als gerade jetzt, da er sein durchnäßtes Haar aus der Stirn strich und der Schein des Kaminfeuers auf seine bleiche Stirn und seine ebenso bleichen Wangen fiel, wo ich heute zum erstenmal die Furchen und Linien entdeckte, welche Kummer und Sorge so deutlich darauf gezogen.\n",
      " PREDICTED: Indessen , wenn er wahnsinnig war , war sein Wahnsinn ein sehr kalter , Moment ; ich hatte niemals dieses hübschen , hübschen Gesicht so hübschen Züge , das er jetzt , gerade so kurz und finster , als er auf dem kahlen , Haar und der Glut , welche meine Wangen jetzt so hoch über dem Kummer , dem Kummer , dem Kummer , welches mich so deutlich und Sorge so deutlich darauf gerichtet hat , welche Kummer , welche Kummer und schwer .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Sinking deeper and deeper into his revery: \"So this,\" he said to himself, following her vaguely with his eyes, \"is la Esmeralda! a celestial creature! a street dancer! so much, and so little!\n",
      "    TARGET: Immer mehr in seine Träume versunken, sagte er sich, als er ihr flüchtig mit den Augen folgte: »Das also ist diese ›Esmeralda‹! Ein himmlisches Geschöpf!\n",
      " PREDICTED: Immer mehr in seine Träume versunken , sagte er sich , als er ihr flüchtig durch die Augen folgte : » Das also ist diese › Esmeralda ‹ ein so kleines Geschöpf !\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Yes, of course I will go,' he decided, lifting his eyes from the book, and a vivid sense of the joy of seeing her made his face radiant.\n",
      "    TARGET: Ich fahre bestimmt hin‹, war das Ergebnis seiner Überlegungen, und nun hob er den Kopf vom Buche in die Höhe. Indem er sich das Glück, sie wiederzusehen, lebhaft vorstellte, strahlte er über das ganze Gesicht.\n",
      " PREDICTED: Ja , ich werde fahren ‹, sagte er zu sich , indem er den Blick von dem Buche in die Höhe hob und in die Freude ihres Gesichtes , das er auf dem Gesicht sah , strahlte vor Freude .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They had probably been standing there ever since K. had opened the door, they avoided seeming to observe K. but chatted lightly and followed his movements with glances, the absent minded glances to the side such as you make during a conversation.\n",
      "    TARGET: Sie standen dort vielleicht schon, seitdem K. die Tür geöffnet hatte, sie vermieden jeden Anschein, als ob sie K. etwa beobachteten, sie unterhielten sich leise und verfolgten K.s Bewegungen mit den Blicken nur so, wie man während eines Gesprächs zerstreut umherblickt.\n",
      " PREDICTED: Sie standen dort vielleicht , seitdem K . die Tür geöffnet hatte , sie vermieden offenbar , daß sie K . sich beobachten ließen ; aber sie folgten leicht und aufmerksam , sie kamen danach , um die Augen zu beobachten , während Sie einen solchen Gegenstand kennen .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was verging on dusk, and the clock had already given warning of the hour to dress for dinner, when little Adele, who knelt by me in the drawing-room window-seat, suddenly exclaimed--\n",
      "    TARGET: Es begann schon zu dämmern, die Glocke hatte bereits das Zeichen zum Ankleiden für die Dinerstunde gegeben, als die kleine Adele, welche neben mir auf einem Sitze in der Fenstervertiefung kniete, plötzlich fröhlich ausrief:\n",
      " PREDICTED: Jetzt begann der Nachmittag zu Ende , und die Uhr hatte noch gerade das Zeichen der Bibliothek gegeben , an Adele , welche neben mir kniete , neben mir auf dem nieder und rief plötzlich :\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: ANNA AND VRONSKY HAD LONG BEEN exchanging glances regretting their friend's clever loquacity, and at last Vronsky without waiting for his host crossed the room to look at another and smaller picture.\n",
      "    TARGET: Anna und Wronski, die die kluge Gesprächigkeit ihres Freundes bedauerten, hatten schon lange miteinander Blicke gewechselt; endlich ging Wronski, ohne eine Aufforderung des Hausherrn abzuwarten, zu einem andern, kleinen Bilde hinüber.\n",
      " PREDICTED: Anna und Wronski hatten sich schon lange miteinander Blicke gewechselt ; es machte sich hier ein ganz Urteil , daß er mit dem Hausherrn , ohne den Wirt zu warten , und zwar zweimal mit ihr gesehen zu werden .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: In the whole world there was only one being able to unite in itself the universe and the meaning of life for him.\n",
      "    TARGET: Es gab auf der ganzen Welt nur ein Wesen, das imstande war, für ihn alles Licht und seinen gesamten Lebensinhalt in sich zusammenzudrängen. Das war sie.\n",
      " PREDICTED: Er hatte doch immer noch das Leben eines jeden Menschen , der in dieser Weise die Köpfe der Welt und seinen Wert um sich hat .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: That leather bed on which so many unhappy wretches had writhed, frightened her.\n",
      "    TARGET: Das lederne Bett, auf dem sich so viele Unglückliche gekrümmt hatten, flößte ihr Entsetzen ein.\n",
      " PREDICTED: Das lederne Bett , auf dem sich so viele Unglückliche gekrümmt hatten , flößte ihr Entsetzen ein .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31231d48",
   "metadata": {},
   "source": [
    "# Checking the tensorboard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b66b1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd3d6f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "TensorBoard 2.20.0 at http://0.0.0.0:6006/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you are on ssh then make sure you are doing port forwarding\n",
    "#ssh -L 6005:localhost:6005 user@jetson_ip\n",
    "#Finally on your host on a browser open http://localhost:6005\n",
    "\n",
    "logdir = \"./runs/tmodel/\"\n",
    "os.system(f\"tensorboard --logdir {logdir} --port 6006 --host 0.0.0.0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cdbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_jetson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
