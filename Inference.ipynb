{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f67ede",
   "metadata": {},
   "source": [
    "# Running Inference\n",
    "\n",
    "This notebook demonstrates how to run inference with both PyTorch and TensorRT models, benchmark their performance, and verify that both models produce similar outputs. It includes:\n",
    "\n",
    "- Loading PyTorch and TensorRT models\n",
    "- Performance benchmarking (latency comparison)\n",
    "- Output verification (comparing translations and numerical outputs)\n",
    "- Encoder output accuracy verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac52c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import get_config, get_weights_file_path\n",
    "from train import get_model, get_ds, run_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce0978",
   "metadata": {},
   "source": [
    "## Let's find the GPU specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d018d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: 1\n",
      "\n",
      "---------------------------\n",
      "\n",
      "All Properties: _CudaDeviceProperties(name='Orin', major=8, minor=7, total_memory=7619MB, multi_processor_count=8, uuid=6c7f4d62-8e83-589a-a8af-556d15b4a582, pci_bus_id=0, pci_device_id=0, pci_domain_id=0, L2_cache_size=2MB)\n",
      "---------------------------\n",
      "\n",
      "--- GPU Index: 0 ---\n",
      "Name: Orin\n",
      "Processor Count: 8\n",
      "Total Memory: 7.44 GB\n",
      "Warp Size: 32\n",
      "Memory Clock Rate: N/A Hz\n",
      "Memory Bus Width: N/A bits\n",
      "Gcn Arch: N/A\n",
      "Compute Capability (arch): 8.7\n",
      "Is Integrated: 1\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_info():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No GPU detected by PyTorch.\")\n",
    "        return\n",
    "    \n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"GPUs detected: {num_devices}\\n\")\n",
    "\n",
    "    for device_idx in range(num_devices):\n",
    "        device_name = torch.cuda.get_device_name(device_idx)\n",
    "        props = torch.cuda.get_device_properties(device_idx)\n",
    "        print(\"---------------------------\\n\")\n",
    "        print(f\"All Properties: {props}\")\n",
    "        print(\"---------------------------\\n\")\n",
    "\n",
    "        print(f\"--- GPU Index: {device_idx} ---\")\n",
    "        print(f\"Name: {device_name}\")\n",
    "        print(f\"Processor Count: {props.multi_processor_count}\")\n",
    "        print(f\"Total Memory: {props.total_memory / (1024 ** 3):.2f} GB\")\n",
    "        #print(f\"Max Threads per Block: {props.max_threads_per_block}\")\n",
    "        print(f\"Warp Size: {props.warp_size}\")\n",
    "        #print(f\"Clock Rate: {props.clock_rate / 1e6:.2f} GHz\")\n",
    "        print(f\"Memory Clock Rate: {getattr(props, 'memory_clock_rate', 'N/A')} Hz\")\n",
    "        print(f\"Memory Bus Width: {getattr(props, 'memory_bus_width', 'N/A')} bits\")\n",
    "        print(f\"Gcn Arch: {getattr(props, 'gcnArch', 'N/A')}\")\n",
    "        print(f\"Compute Capability (arch): \"\n",
    "              f\"{getattr(props, 'major', 'N/A')}.{getattr(props, 'minor', 'N/A')}\")\n",
    "        print(f\"Is Integrated: {props.is_integrated}\")\n",
    "        print(\"---------------------------\\n\")\n",
    "\n",
    "get_gpu_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098d6eb",
   "metadata": {},
   "source": [
    "## Setup: Device and Data Loading\n",
    "\n",
    "Initialize the device (CUDA if available), load configuration, and prepare the data loaders and tokenizers for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2d1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "The dataset en-de is not available.\n",
      "Checking availability of de-en...\n",
      "Loaded dataset with config: de-en\n",
      "Max length of source sentence: 466\n",
      "Max length of target sentence: 479\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bfdf99",
   "metadata": {},
   "source": [
    "## Load PyTorch Model\n",
    "\n",
    "Load the trained PyTorch transformer model from the checkpoint. The model is moved to the specified device (CUDA/CPU) and weights are loaded from the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd12198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = \"29\"\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = get_weights_file_path(config, EPOCH)\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "del state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34b7ae",
   "metadata": {},
   "source": [
    "## Prepare for Inference\n",
    "\n",
    "Set the model to evaluation mode and disable gradient computation for faster inference. Import libraries needed for benchmarking (time, numpy, pandas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edaeeaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bb38b",
   "metadata": {},
   "source": [
    "## Load TensorRT Model\n",
    "\n",
    "Load the split TensorRT engines (encoder, decoder, and projection layers). The memory fraction is limited to 60% to ensure TensorRT has enough GPU memory. The engines are loaded from the `tensorrt_split/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59c40dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRT Engines...\n",
      "Engines loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<run_trt_split.TRTTransformer at 0xfffeb81822f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: this import should NOT trigger inference because the file uses if __name__ == \"__main__\"\n",
    "from run_trt_split import TRTTransformer, greedy_decode as greedy_decode_trt\n",
    "\n",
    "# (optional) same memory fraction trick in notebook\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_per_process_memory_fraction(0.6)\n",
    "\n",
    "trt_model = TRTTransformer(\n",
    "    enc_path=\"tensorrt_split/tmodel_\" + EPOCH + \"_encoder_fp32.engine\",\n",
    "    dec_path=\"tensorrt_split/tmodel_\" + EPOCH + \"_decoder_fp32.engine\",\n",
    "    proj_path=\"tensorrt_split/tmodel_\" + EPOCH + \"_projection_fp32.engine\"\n",
    ")\n",
    "\n",
    "trt_model  # sanity: should print \"Engines loaded.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431ebf4",
   "metadata": {},
   "source": [
    "## Import Decoding Functions\n",
    "\n",
    "Import the greedy decoding function from the training module for PyTorch inference. The TensorRT version was already imported above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21fa68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import greedy_decode as greedy_decode_pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a9e79",
   "metadata": {},
   "source": [
    "## Benchmark Function\n",
    "\n",
    "The `benchmark_decode` function measures inference latency for a given model and decoding function. It:\n",
    "\n",
    "1. **Warmup Phase**: Runs a few inference passes to stabilize GPU performance and optionally prints sample translations\n",
    "2. **Timed Phase**: Measures actual inference time for multiple batches\n",
    "3. **Statistics**: Computes mean, median (p50), p90, and p99 percentiles of latency\n",
    "\n",
    "This function is used to compare PyTorch vs TensorRT performance.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a711112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_decode(\n",
    "    decode_fn,\n",
    "    model_obj,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"\",\n",
    "    print_warmup_samples=0 \n",
    "):\n",
    "    times_ms = []\n",
    "\n",
    "    # ---------------- Warmup (optional prints here) ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= warmup_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        out_ids = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "        if i < print_warmup_samples:\n",
    "            out_text = tokenizer_tgt.decode(out_ids.detach().cpu().numpy())\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{label} WARMUP SAMPLE {i+1}\")\n",
    "            print(f\"SOURCE:    {batch['src_text'][0]}\")\n",
    "            print(f\"TARGET:    {batch['tgt_text'][0]}\")\n",
    "            print(f\"PRED:      {out_text}\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # ---------------- Timed runs ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        _ = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        times_ms.append((t1 - t0) * 1000)\n",
    "\n",
    "    arr = np.array(times_ms, dtype=np.float32)\n",
    "    print(f\"\\n{label} latency over {len(arr)} batches:\")\n",
    "    print(f\"  mean: {arr.mean():.2f} ms\")\n",
    "    print(f\"  p50 : {np.percentile(arr, 50):.2f} ms\")\n",
    "    print(f\"  p90 : {np.percentile(arr, 90):.2f} ms\")\n",
    "    print(f\"  p99 : {np.percentile(arr, 99):.2f} ms\")\n",
    "\n",
    "    return times_ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d67ad",
   "metadata": {},
   "source": [
    "## Run Performance Benchmarks\n",
    "\n",
    "Benchmark both PyTorch and TensorRT models on the validation dataset. This will:\n",
    "\n",
    "- Run 5 warmup batches (with 2 sample translations printed)\n",
    "- Measure latency for 50 batches\n",
    "- Print latency statistics (mean, p50, p90, p99)\n",
    "- Calculate the speedup factor (PyTorch latency / TensorRT latency)\n",
    "\n",
    "The results show how much faster TensorRT is compared to PyTorch for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5eabe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 1\n",
      "SOURCE:    * The city of Cambrai is well dressed. Marafin plundered it.\n",
      "TARGET:    Was für schöne Kleider hat Cambrai, die gute Stadt: Marasin hat sie geplündert.\n",
      "PRED:      Was für schöne Kleider hat , die gute Stadt : hat sie der guten Stadt bekommen .\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 2\n",
      "SOURCE:    Upon which the bishop had been constrained to recite to him the ordinance of Legate Odo, which excepts certain great dames, ~aliquoe magnates mulieres, quoe sine scandalo vitari non possunt~.\n",
      "TARGET:    Daraufhin hatte ihm der Bischof die Verordnung des Legaten Odo citiren müssen, welche gewisse Damen von Stande »aliquae magnates mulieres, quae sine scandalo evitari non possunt« ausnimmt.\n",
      "PRED:      Bei dem aber , welcher der Bischof die Verordnung des Bischofs hatte , wie ihn die » « jenen Damen « nannte : » , quae sine non .«\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 3\n",
      "SOURCE:    'Oh, I understand nothing about it!\n",
      "TARGET:    »Ach, ich verstehe von diesen Dingen nichts!\n",
      "PRED:      » Ach , ich verstehe von diesen Dingen nichts !\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 4\n",
      "SOURCE:    \"I don't think you have, Bessie.\"\n",
      "TARGET:    »Ach nein, Bessie, das hast du nicht.« »Kind!\n",
      "PRED:      » O nein , Bessie , das hast du nicht .«\n",
      "\n",
      "PyTorch latency over 50 batches:\n",
      "  mean: 939.68 ms\n",
      "  p50 : 754.54 ms\n",
      "  p90 : 1764.37 ms\n",
      "  p99 : 2815.76 ms\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 1\n",
      "SOURCE:    'Anna and – vice... I cannot combine them, I cannot believe it!'\n",
      "TARGET:    »Anna und die Sünde – das kann ich nicht vereinigen, das kann ich nicht glauben.«\n",
      "PRED:      » Anna und die Sünde – das kann ich nicht vereinigen , das kann ich nicht glauben .«\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 2\n",
      "SOURCE:    \"That's bully. Plenty bully enough for me.\n",
      "TARGET:    ,,Wär' grad' was für mich, Tom, wär' ganz extra was für mich!\n",
      "PRED:      ,, Aber , darüber war ' s nicht schwer , mich zu beruhigen .\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 3\n",
      "SOURCE:    Intimate attachment!\"\n",
      "TARGET:    Herzliche Liebe und Anhänglichkeit!«\n",
      "PRED:      Liebe und Anhänglichkeit !«\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 4\n",
      "SOURCE:    A dream had scarcely approached my ear, when it fled affrighted, scared by a marrow-freezing incident enough.\n",
      "TARGET:    Kaum hatte ein Traum sich leise flüsternd meinem Ohre genähert, als er erschreckt von dannen floh, von einem markerschütternden Zwischenfall verjagt.\n",
      "PRED:      Kaum war ein Traum sich meinem Ohre genähert , als er erschreckt von dannen entfloh .\n",
      "\n",
      "TensorRT latency over 50 batches:\n",
      "  mean: 519.43 ms\n",
      "  p50 : 411.15 ms\n",
      "  p90 : 962.69 ms\n",
      "  p99 : 1629.37 ms\n",
      "\n",
      "Speedup (PyTorch / TRT): 1.81x\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_len = config[\"seq_len\"]\n",
    "\n",
    "pt_times = benchmark_decode(\n",
    "    greedy_decode_pt, model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"PyTorch\",\n",
    "    print_warmup_samples=4\n",
    ")\n",
    "\n",
    "trt_times = benchmark_decode(\n",
    "    greedy_decode_trt, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"TensorRT\",\n",
    "    print_warmup_samples=4\n",
    ")\n",
    "\n",
    "speedup = np.mean(pt_times) / np.mean(trt_times)\n",
    "print(f\"\\nSpeedup (PyTorch / TRT): {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962067f",
   "metadata": {},
   "source": [
    "| GPU        | Runtime Screenshot | GPU Details Screenshot |\n",
    "|------------|------------------|----------------------|\n",
    "| **MI300X** | ![MI300X Runtime](./images/rocm_runtime_pytorch.png) | ![MI300X Details](./images/amd_gpu_details.png) |\n",
    "| **L4**     | ![L4 Runtime](./images/l4_runtime.png) | ![L4 Details](./images/l4_details.png) |\n",
    "| **A100**   | ![A100 Runtime](./images/a100_runtime.png) | ![A100 Details](./images/a100_details.png) |\n",
    "| **Jetson Orin Nano Super**   | ![Jetson Runtime](./images/jetson_runtime.png) | ![Jetson Details](./images/jetson_details.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f0bc4",
   "metadata": {},
   "source": [
    "## Translation Comparison\n",
    "\n",
    "The `verbose_compare_same_sample` function compares translations from both models on the same input sample. It:\n",
    "\n",
    "- Takes the same batch from the dataloader\n",
    "- Runs inference with both PyTorch and TensorRT models\n",
    "- Prints side-by-side comparison of source, target, and predictions\n",
    "- Returns the token IDs from both models\n",
    "\n",
    "This helps verify that both models produce similar (or identical) translations.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8940670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE:     When the Prince flared up she kept silent, feeling shame for her mother and tenderness toward her father because of his immediate return to kindliness; but when her father left the room she was ready for the chief thing needful, which was to go to Kitty and comfort her.\n",
      "TARGET:     Während dann der Fürst seinem Ingrimm Luft machte, hatte sie geschwiegen und sich für ihre Mutter geschämt; mit zärtlicher Bewunderung hatte sie auf ihren Vater geblickt, als dieser so bald wieder gut und freundlich wurde. Aber als nun der Vater hinausgegangen war, da schickte sie sich an, das Wichtigste zu tun, was jetzt nötig war: zu Kitty zu gehen und sie zu beruhigen.\n",
      "PT  PRED:   Beim Anblick des hatte sie sich gekränkt ; aber als sie das Schweigen der Mutter und mit einer Neigung für den Vater dieses Namens dieser Neigung zur Ruhe des Vaters zurückkehren ; aber als sie das Zimmer verlassen hatte , da hatte sie für Kitty alles bereit , zu kommen , was Kitty zu tun habe , um sie zu beruhigen , und die Mutter zu beruhigen .\n",
      "TRT PRED:   Beim Anblick des hatte sie sich gekränkt ; aber als sie das Schweigen der Mutter und mit einer Neigung für den Vater dieses Namens dieser Neigung zur Ruhe des Vaters zurückkehren ; aber als sie das Zimmer verlassen hatte , da hatte sie für Kitty alles bereit , zu kommen , was Kitty zu tun habe , um sie zu beruhigen , und die Mutter zu beruhigen .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def verbose_compare_same_sample(\n",
    "    pt_model,\n",
    "    trt_model,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    batch_index=0,   # pick which batch to compare\n",
    "):\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---- get ONE specific batch ----\n",
    "    it = iter(dataloader)\n",
    "    batch = None\n",
    "    for i in range(batch_index + 1):\n",
    "        batch = next(it)\n",
    "\n",
    "    encoder_input = batch[\"encoder_input\"].to(device)\n",
    "    encoder_mask  = batch[\"encoder_mask\"].to(device)\n",
    "    source_text   = batch[\"src_text\"][0]\n",
    "    target_text   = batch[\"tgt_text\"][0]\n",
    "\n",
    "    # ---- PyTorch decode on SAME tensors ----\n",
    "    out_pt_ids = greedy_decode_pt(\n",
    "        pt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_pt_text = tokenizer_tgt.decode(out_pt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- TensorRT decode on SAME tensors ----\n",
    "    out_trt_ids = greedy_decode_trt(\n",
    "        trt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_trt_text = tokenizer_tgt.decode(out_trt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- print SAME sample ----\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"SOURCE:     {source_text}\")\n",
    "    print(f\"TARGET:     {target_text}\")\n",
    "    print(f\"PT  PRED:   {out_pt_text}\")\n",
    "    print(f\"TRT PRED:   {out_trt_text}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    return out_pt_ids, out_trt_ids\n",
    "\n",
    "\n",
    "# run on batch 0 (same sample)\n",
    "_ = verbose_compare_same_sample(\n",
    "    model, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    config[\"seq_len\"], device,\n",
    "    batch_index=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97e13c",
   "metadata": {},
   "source": [
    "## Logits Comparison (First Decoding Step)\n",
    "\n",
    "The `compare_first_step_logits` function verifies numerical accuracy by comparing the logits from the first decoding step. It:\n",
    "\n",
    "- Encodes the source sequence with both models\n",
    "- Runs one decoder step (with [SOS] token) for both models\n",
    "- Compares the output logits (vocabulary probabilities)\n",
    "- Reports maximum and mean absolute differences\n",
    "\n",
    "Small differences (< 0.01) indicate that TensorRT is producing numerically similar outputs to PyTorch, which is expected due to floating-point precision differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6fdf7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-step logits diff:\n",
      "  max abs: 0.007963895797729492\n",
      "  mean abs: 0.0017605420434847474\n"
     ]
    }
   ],
   "source": [
    "from dataset import causal_mask\n",
    "def compare_first_step_logits(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    # PT: encoder + one decoder step (SOS)\n",
    "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "    dec_in = torch.tensor([[sos_idx]], device=device, dtype=src.dtype)\n",
    "\n",
    "    enc_pt = pt_model.encode(src, src_mask)\n",
    "    dec_mask = causal_mask(1).type_as(src_mask).to(device)  # (1,1,1)\n",
    "    out_pt = pt_model.decode(enc_pt, src_mask, dec_in, dec_mask)\n",
    "    logits_pt = pt_model.project(out_pt[:, -1])  # (B, vocab)\n",
    "\n",
    "    # TRT: same\n",
    "    enc_trt = trt_model.encode(src, src_mask)\n",
    "    dec_mask_trt = causal_mask(1).type_as(src_mask).to(device).unsqueeze(1)  # (1,1,1,1)\n",
    "    out_trt = trt_model.decode(enc_trt, src_mask, dec_in, dec_mask_trt)\n",
    "    logits_trt = trt_model.project(out_trt[:, -1])\n",
    "\n",
    "    lp = logits_pt.detach().cpu().float()\n",
    "    lt = logits_trt.detach().cpu().float()\n",
    "\n",
    "    max_abs = (lp - lt).abs().max().item()\n",
    "    mean_abs = (lp - lt).abs().mean().item()\n",
    "    print(\"First-step logits diff:\")\n",
    "    print(\"  max abs:\", max_abs)\n",
    "    print(\"  mean abs:\", mean_abs)\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_first_step_logits(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0b996",
   "metadata": {},
   "source": [
    "## Encoder Output Comparison\n",
    "\n",
    "The `compare_encoder_outputs` function verifies that the encoder produces similar outputs in both models. It:\n",
    "\n",
    "- **`pt_encode_trt_style`**: Adapts PyTorch encoder to use the same mask format as TensorRT (square mask shape)\n",
    "- **`compare_encoder_outputs`**: Compares encoder outputs from both models and reports maximum and mean absolute differences\n",
    "\n",
    "This ensures that the encoder component is working correctly in the TensorRT version. Differences should be very small (< 0.001) due to floating-point precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c220954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER diff: max = 0.000666201114654541 mean = 5.592039451585151e-05\n"
     ]
    }
   ],
   "source": [
    "def pt_encode_trt_style(pt_model, src, src_mask):\n",
    "    # Make PT use the same square mask shape TRT encoder expects\n",
    "    S = src.shape[1]\n",
    "    if src_mask.dim() == 3:\n",
    "        src_mask = src_mask.unsqueeze(1)   # (B,1,S)->(B,1,1,S)\n",
    "    if src_mask.dim() == 4 and src_mask.shape[2] == 1:\n",
    "        src_mask = src_mask.repeat(1,1,S,1)  # -> (B,1,S,S)\n",
    "    src_mask = src_mask.float()  # binary float\n",
    "    return pt_model.encode(src, src_mask)\n",
    "\n",
    "def compare_encoder_outputs(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    enc_pt  = pt_encode_trt_style(pt_model, src, src_mask).detach().cpu().float()\n",
    "    enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "    diff = (enc_pt - enc_trt).abs()\n",
    "    print(\"ENCODER diff: max =\", diff.max().item(), \"mean =\", diff.mean().item())\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_encoder_outputs(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8351e3",
   "metadata": {},
   "source": [
    "## Position-wise Encoder Analysis\n",
    "\n",
    "This cell performs a detailed analysis of encoder output differences:\n",
    "\n",
    "- Computes per-position mean differences across the hidden dimension\n",
    "- Separates differences for **padded positions** (where mask = 0) vs **unpadded positions** (where mask = 1)\n",
    "- Reports mean differences for each category and the overall maximum\n",
    "\n",
    "This helps understand if differences are concentrated in padded regions (which are typically ignored) or in actual content positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96bb3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean diff padded   : 4.1217113903257996e-05\n",
      "mean diff unpadded : 1.9660077668959275e-05\n",
      "max diff overall   : 0.000134581932798028\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "batch0 = next(iter(val_dataloader))\n",
    "src = batch0[\"encoder_input\"].to(device)\n",
    "src_mask = batch0[\"encoder_mask\"].to(device)\n",
    "\n",
    "enc_pt  = pt_encode_trt_style(model, src, src_mask).detach().cpu().float()\n",
    "enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "# per-position mean diff across hidden dim\n",
    "pos_diff = (enc_pt - enc_trt).abs().mean(-1).squeeze(0)   # (S,)\n",
    "\n",
    "# padded vs unpadded positions (mask is (1,1,1,S))\n",
    "key_mask = src_mask.squeeze().cpu()  # (S,) with 0/1\n",
    "pad_pos = key_mask == 0\n",
    "unpad_pos = key_mask == 1\n",
    "\n",
    "print(\"mean diff padded   :\", pos_diff[pad_pos].mean().item())\n",
    "print(\"mean diff unpadded :\", pos_diff[unpad_pos].mean().item())\n",
    "print(\"max diff overall   :\", pos_diff.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a98e9",
   "metadata": {},
   "source": [
    "## Export Benchmark Results\n",
    "\n",
    "Save the benchmark timing data to a CSV file for further analysis. The DataFrame contains:\n",
    "\n",
    "- `pytorch_ms`: Latency for each PyTorch inference (milliseconds)\n",
    "- `tensorrt_ms`: Latency for each TensorRT inference (milliseconds)\n",
    "- `speedup_x`: Per-sample speedup ratio (PyTorch / TensorRT)\n",
    "\n",
    "The `describe()` method provides summary statistics (mean, std, min, max, percentiles) for all columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3569b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = min(len(pt_times), len(trt_times))\n",
    "df = pd.DataFrame({\n",
    "    \"pytorch_ms\": pt_times[:m],\n",
    "    \"tensorrt_ms\": trt_times[:m],\n",
    "})\n",
    "df[\"speedup_x\"] = df[\"pytorch_ms\"] / df[\"tensorrt_ms\"]\n",
    "\n",
    "df.to_csv(\"benchmark_times.csv\", index=False)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36673b4b",
   "metadata": {},
   "source": [
    "## Optional: Full Validation\n",
    "\n",
    "Uncomment the line below to run full validation on the PyTorch model. This will print multiple translation examples with source, target, and predicted outputs. This is useful for qualitative assessment of translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b72c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Benserade prepared us for it by some very gallant verses.\"\n",
      "    TARGET: Benserade bereitete uns in recht niedlichen Versen darauf vor.«\n",
      " PREDICTED: bereitete uns in recht niedlichen Versen darauf vor .«\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She renounced everything. But facts and time have shown that her situation is tormenting and impossible.'\n",
      "    TARGET: Aber das nüchterne Leben und die Zeit haben ihr bewiesen, daß ihre Lage qualvoll und unerträglich ist.«\n",
      " PREDICTED: Aber das Volk hat alles geordnet , alles ist die Zeit , die sie hat , eine Qual und unmöglich .«\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She came up to him and said, 'My darling!'\n",
      "    TARGET: »Mein lieber Junge!« sagte sie.\n",
      " PREDICTED: Sie trat zu ihm heran und sagte : › Meine Liebe !‹\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: More than this; the brave chevalier had obtained the reversion of the office for his son, and for two years already, the name of the noble man Jacques d'Estouteville, equerry, had figured beside his at the head of the register of the salary list of the provostship of Paris.\n",
      "    TARGET: Noch mehr: der tapfere Ritter hatte für seinen Sohn die Anwartschaft auf sein Amt erhalten, und schon seit zwei Jahren prangte der Name des edeln Herrn Jacob von Estouteville, des Junkers, neben dem seinigen am Kopfe des Civilregisters des Pariser Gerichtsamtes.\n",
      " PREDICTED: Mehr als mehr : der tapfere Ritter hatte für seinen Sohn von seinen Sohn und für zwei Jahren den Namen des , der Fürst von Estouteville mit einem frommen Menschen , am Kopfe und dem Kopfe des , das von Paris im Jahre des heiligen .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Oh!\" exclaimed Coppenole, whose eyes suddenly flashed, \"that reminds me of the burning of the house of the Seigneur d'Hymbercourt.\n",
      "    TARGET: »Ho!« fügte Coppenole hinzu, dessen Augen plötzlich aufleuchteten, »das erinnert mich an den Brand des Hauses des Herrn von Hymbercourt.\n",
      " PREDICTED: » Ach !« rief Coppenole , dessen Augen plötzlich funkelten ; » was erinnert mich an den Herrn des Hauses von Dienern des Hauses ' Wohnzimmer ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Please don't think I am condemning you. Not at all!\n",
      "    TARGET: Vielleicht hätte ich an ihrer Stelle dasselbe getan.\n",
      " PREDICTED: » Bitte , glauben Sie nicht , daß ich das werde .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Ho! ho!\" said Tristan to the soldier, \"you have the nose of an inquisitor of the Châtelet.\n",
      "    TARGET: »Ei! ei!« sagte Tristan zu dem Soldaten, »du hast eine Nase wie ein Untersuchungsrichter im Châtelet.\n",
      " PREDICTED: » Ei ! ei !« sagte Tristan zu dem Soldat , » du hast eine Nase wie ein Untersuchungsrichter im Châtelet .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Let us pray to the Lord that He may send them perfect love, peace, and help!' the whole church seemed to breathe with the senior deacon's voice.\n",
      "    TARGET: »Daß er ihnen herniedersende die allervollkommenste, allerfriedlichste Liebe und seine Hilfe, das erbitten wir von Gott!« sang der Protodiakon, und das ganze Kirchengebäude schien mitzusingen.\n",
      " PREDICTED: » Laßt uns das Geld nehmen , damit er ihnen die Liebe schickt , und die ganze Kirche !« sang der Protodiakon , die ganze Kirche wie schien .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Oh! what a fine noise! ~Populi debacchantis populosa debacchatio~!\"\n",
      "    TARGET: »Ach! ein köstlicher Lärm!\n",
      " PREDICTED: » Ach ! ein schönes Geräusch ! – !«\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: K. thought no more about the matter, he merely watched the immediate effect of the deputy director's appearance and, for him, the effect was very pleasing; the manufacturer immediately jumped up from his seat and hurried over to meet the deputy director, although K. would have liked to make him ten times livelier as he feared the deputy director might disappear again.\n",
      "    TARGET: Denn sofort hüpfte der Fabrikant vom Sessel auf und eilte dem Direktor-Stellvertreter entgegen, K. aber hätte ihn noch zehnmal flinker machen wollen, denn er fürchtete, der Direktor-Stellvertreter könnte wieder verschwinden.\n",
      " PREDICTED: K . dachte nicht mehr daran , er suchte nur noch den Blicken des Direktor - Stellvertreters , es war ihm gleich sehr vorgekommen , K . durch die Tatsache des Direktor - Stellvertreters , der doch noch zehn Mal , daß der Direktor - Stellvertreter , vielleicht auch K . wieder sehr fürchtete , daß er den Direktor - Stellvertreter könnte , vielleicht auch nur das Direktor - Stellvertreter könnte , der doch noch zehn Mal Furcht zurückhalten würde .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31231d48",
   "metadata": {},
   "source": [
    "# Checking the tensorboard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b66b1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd3d6f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "TensorBoard 2.20.0 at http://0.0.0.0:6006/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you are on ssh then make sure you are doing port forwarding\n",
    "#ssh -L 6005:localhost:6005 user@jetson_ip\n",
    "#Finally on your host on a browser open http://localhost:6005\n",
    "\n",
    "logdir = \"./runs/tmodel/\"\n",
    "os.system(f\"tensorboard --logdir {logdir} --port 6006 --host 0.0.0.0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cdbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_jetson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
